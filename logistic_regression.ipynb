{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muvVJKXl5G6D",
        "outputId": "f8a4a611-3a73-48e8-e7f1-5bef11b7d242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world!\n"
          ]
        }
      ],
      "source": [
        "print(\"hello world!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory"
      ],
      "metadata": {
        "id": "7E5OZhx7533-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "\n",
        "ans. ### **Logistic Regression vs. Linear Regression**  \n",
        "\n",
        "#### **Logistic Regression**  \n",
        "Logistic Regression is a **classification algorithm** used to predict the probability of a categorical outcome, typically for **binary classification** (e.g., spam vs. not spam). It uses the **sigmoid (logistic) function** to output probabilities between 0 and 1, which are then mapped to class labels.\n",
        "\n",
        "#### **Linear Regression**  \n",
        "Linear Regression, on the other hand, is a **regression algorithm** used to predict **continuous numerical values** (e.g., predicting house prices). It fits a straight line to model the relationship between input variables and the output.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences**\n",
        "| Feature             | Logistic Regression | Linear Regression |\n",
        "|---------------------|---------------------|-------------------|\n",
        "| **Type of Problem** | Classification | Regression |\n",
        "| **Output Type** | Probability (0-1) | Continuous Value |\n",
        "| **Function Used** | Sigmoid Function | Linear Function |\n",
        "| **Loss Function** | Log Loss (Cross-Entropy) | Mean Squared Error (MSE) |\n",
        "| **Interpretation** | Probability of belonging to a class | Best-fit line prediction |\n",
        "\n",
        "**When to Use What?**  \n",
        "- Use **Logistic Regression** when predicting categorical outcomes (Yes/No, Fraud/Not Fraud).\n",
        "- Use **Linear Regression** when predicting numerical values (Sales, Temperature, Prices)."
      ],
      "metadata": {
        "id": "S9Dhel4_573b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is the mathematical equation of Logistic Regression.\n",
        "\n",
        "ans. ### **Mathematical Equation of Logistic Regression**  \n",
        "\n",
        "Logistic Regression models the probability that a given input **\\(X\\)** belongs to class **\\(Y = 1\\)** using the **sigmoid function**:\n",
        "\n",
        "\\[\n",
        "P(Y=1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + ... + \\beta_nX_n)}}\n",
        "\\]\n",
        "\n",
        "where:  \n",
        "- \\( P(Y=1 | X) \\) is the probability of the instance belonging to class 1.  \n",
        "- \\( \\beta_0 \\) (intercept) and \\( \\beta_1, \\beta_2, ..., \\beta_n \\) (coefficients) are learned parameters.  \n",
        "- \\( X_1, X_2, ..., X_n \\) are feature variables.  \n",
        "- \\( e \\) is Euler‚Äôs number (~2.718).  \n",
        "\n",
        "### **Log-Odds Form**\n",
        "Taking the **log-odds transformation**, we rewrite the equation as:\n",
        "\n",
        "\\[\n",
        "\\log \\left( \\frac{P(Y=1 | X)}{1 - P(Y=1 | X)} \\right) = \\beta_0 + \\beta_1X_1 + ... + \\beta_nX_n\n",
        "\\]\n",
        "\n",
        "This shows that Logistic Regression models a **linear relationship** between the **log-odds** of the dependent variable and the independent variables."
      ],
      "metadata": {
        "id": "zTiGN7zvBjv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "ans. ### **Why Do We Use the Sigmoid Function in Logistic Regression?**  \n",
        "\n",
        "The **Sigmoid function** (also called the **logistic function**) is used in Logistic Regression to map any real-valued number into a range between **0 and 1**, making it useful for probability estimation.\n",
        "\n",
        "#### **Sigmoid Function Formula**  \n",
        "\\[\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\]\n",
        "where:  \n",
        "- \\( z = \\beta_0 + \\beta_1X_1 + ... + \\beta_nX_n \\) (linear combination of input features).  \n",
        "- \\( e \\) is Euler‚Äôs number (~2.718).  \n",
        "\n",
        "### **Why is it Used?**\n",
        "1. **Probability Interpretation:**  \n",
        "   - The sigmoid function converts raw model outputs into probabilities between **0 and 1**, making it ideal for **binary classification**.\n",
        "  \n",
        "2. **Threshold-Based Classification:**  \n",
        "   - If \\( \\sigma(z) \\geq 0.5 \\), predict **Class 1**.\n",
        "   - If \\( \\sigma(z) < 0.5 \\), predict **Class 0**.\n",
        "\n",
        "3. **Differentiability for Optimization:**  \n",
        "   - The function is smooth and differentiable, allowing gradient-based optimization (like **Gradient Descent**) to update model weights.\n",
        "\n",
        "4. **Handles Extreme Inputs Gracefully:**  \n",
        "   - Unlike a linear function, the sigmoid function **asymptotically** approaches 0 and 1, preventing extreme outputs."
      ],
      "metadata": {
        "id": "a-Om_zQ2Byup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is the cost function of Logistic Regression.\n",
        "\n",
        "ans. ### **Cost Function of Logistic Regression**  \n",
        "\n",
        "Unlike **Linear Regression**, which uses **Mean Squared Error (MSE)**, Logistic Regression uses the **Log Loss (Cross-Entropy Loss)** as its cost function.  \n",
        "\n",
        "### **Why Not Use MSE?**\n",
        "- MSE produces a **non-convex** loss function for Logistic Regression, making it difficult to optimize using Gradient Descent.\n",
        "- Log Loss ensures a **convex** function, leading to better optimization.\n",
        "\n",
        "---\n",
        "\n",
        "### **Log Loss (Cross-Entropy Loss) Function**\n",
        "For **binary classification**, the cost function is:\n",
        "\n",
        "\\[\n",
        "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log (h_\\theta(x_i)) + (1 - y_i) \\log (1 - h_\\theta(x_i)) \\right]\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\( m \\) = number of training examples\n",
        "- \\( y_i \\) = actual class label (0 or 1)\n",
        "- \\( h_\\theta(x_i) \\) = predicted probability from the **sigmoid function**\n",
        "- \\( \\theta \\) = model parameters (weights & bias)\n",
        "\n",
        "---\n",
        "\n",
        "### **How Does It Work?**\n",
        "- If \\( y_i = 1 \\), the cost function simplifies to:  \n",
        "  \\[\n",
        "  -\\log (h_\\theta(x_i))\n",
        "  \\]\n",
        "  - If \\( h_\\theta(x_i) \\) is close to 1 ‚Üí **Low cost**\n",
        "  - If \\( h_\\theta(x_i) \\) is close to 0 ‚Üí **High cost** (penalizes incorrect predictions)\n",
        "\n",
        "- If \\( y_i = 0 \\), the cost function simplifies to:  \n",
        "  \\[\n",
        "  -\\log (1 - h_\\theta(x_i))\n",
        "  \\]\n",
        "  - If \\( h_\\theta(x_i) \\) is close to 0 ‚Üí **Low cost**\n",
        "  - If \\( h_\\theta(x_i) \\) is close to 1 ‚Üí **High cost**\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is Log Loss Used?**\n",
        "‚úî Ensures **convexity** (easier optimization using Gradient Descent).  \n",
        "‚úî **Penalizes incorrect confident predictions** heavily.  \n",
        "‚úî Helps models converge efficiently."
      ],
      "metadata": {
        "id": "Jfy-4jMjB_H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What is Regularization in Logistic Regression? Why is it needed.\n",
        "\n",
        "ans. ### **Regularization in Logistic Regression**  \n",
        "\n",
        "**Regularization** is a technique used to prevent **overfitting** in Logistic Regression by adding a penalty term to the cost function. This helps in keeping the model **simpler** and improves its **generalization** to unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is Regularization Needed?**\n",
        "1. **Prevents Overfitting:**  \n",
        "   - When a model is too complex (many features, high coefficients), it fits the training data too well but fails on new data.  \n",
        "   - Regularization **shrinks the coefficients**, reducing model complexity.  \n",
        "\n",
        "2. **Improves Generalization:**  \n",
        "   - A less complex model performs better on unseen data.  \n",
        "\n",
        "3. **Controls Multicollinearity:**  \n",
        "   - When features are highly correlated, regularization reduces the impact of redundant features.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Types of Regularization in Logistic Regression**\n",
        "Regularization is controlled by the **regularization parameter (Œª or C)** in the cost function.\n",
        "\n",
        "#### **1. L1 Regularization (Lasso Regression)**\n",
        "- Uses the **absolute value** of coefficients:  \n",
        "  \\[\n",
        "  J(\\theta) = - \\sum [y \\log(h_\\theta) + (1 - y) \\log(1 - h_\\theta)] + \\lambda \\sum |\\theta_j|\n",
        "  \\]\n",
        "- Leads to **sparse models** by forcing some coefficients to be exactly **zero** (feature selection).  \n",
        "- Useful when you suspect that **only a few features** are important.  \n",
        "\n",
        "‚úÖ **Used when feature selection is needed.**  \n",
        "\n",
        "---\n",
        "\n",
        "#### **2. L2 Regularization (Ridge Regression)**\n",
        "- Uses the **square** of coefficients:  \n",
        "  \\[\n",
        "  J(\\theta) = - \\sum [y \\log(h_\\theta) + (1 - y) \\log(1 - h_\\theta)] + \\lambda \\sum \\theta_j^2\n",
        "  \\]\n",
        "- Shrinks all coefficients but **does not set them to zero**.  \n",
        "- Useful when all features contribute **somewhat** to the prediction.  \n",
        "\n",
        "‚úÖ **Used when all features are relevant but need to be controlled.**  \n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Elastic Net Regularization**\n",
        "- **Combination of L1 & L2**:  \n",
        "  \\[\n",
        "  J(\\theta) = - \\sum [y \\log(h_\\theta) + (1 - y) \\log(1 - h_\\theta)] + \\lambda_1 \\sum |\\theta_j| + \\lambda_2 \\sum \\theta_j^2\n",
        "  \\]\n",
        "- Keeps the benefits of **Lasso (feature selection)** and **Ridge (small coefficient values)**.  \n",
        "\n",
        "‚úÖ **Used when there are many correlated features.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **Effect of Regularization Parameter (Œª or C)**\n",
        "- **High Œª** (strong regularization) ‚Üí Coefficients shrink more ‚Üí **Simple model**  \n",
        "- **Low Œª** (weak regularization) ‚Üí Coefficients remain larger ‚Üí **Complex model**  \n",
        "- **C = 1/Œª** (used in sklearn) ‚Üí **Higher C means less regularization**  "
      ],
      "metadata": {
        "id": "GwVUfdyOCL6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "ans. ### **Difference Between Lasso, Ridge, and Elastic Net Regression**  \n",
        "\n",
        "Regularization techniques help prevent **overfitting** in Logistic Regression by adding a penalty term to the cost function. The three main types are **Lasso (L1), Ridge (L2), and Elastic Net (L1 + L2)**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **1. Lasso Regression (L1 Regularization)**\n",
        "üîπ **Penalty Term:**  \n",
        "\\[\n",
        "\\lambda \\sum |\\theta_j|\n",
        "\\]\n",
        "üîπ **Effect:**  \n",
        "- Shrinks some **coefficients to exactly zero**, effectively performing **feature selection**.  \n",
        "- Helps when there are many **irrelevant** features.  \n",
        "\n",
        "üîπ **Use Case:**  \n",
        "‚úÖ When feature selection is important (e.g., high-dimensional datasets).  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Ridge Regression (L2 Regularization)**\n",
        "üîπ **Penalty Term:**  \n",
        "\\[\n",
        "\\lambda \\sum \\theta_j^2\n",
        "\\]\n",
        "üîπ **Effect:**  \n",
        "- Shrinks **all coefficients** but **never makes them zero**.  \n",
        "- Helps when **all features contribute**, even if slightly.  \n",
        "\n",
        "üîπ **Use Case:**  \n",
        "‚úÖ When all features are useful and multicollinearity needs to be reduced.  \n",
        "\n",
        "---\n",
        "\n",
        "### **3. Elastic Net Regression (L1 + L2 Regularization)**\n",
        "üîπ **Penalty Term:**  \n",
        "\\[\n",
        "\\lambda_1 \\sum |\\theta_j| + \\lambda_2 \\sum \\theta_j^2\n",
        "\\]\n",
        "üîπ **Effect:**  \n",
        "- **Combines** the benefits of **Lasso (feature selection)** and **Ridge (shrinkage)**.  \n",
        "- Useful when **some features are irrelevant** and **some are correlated**.  \n",
        "\n",
        "üîπ **Use Case:**  \n",
        "‚úÖ When the dataset has **many correlated features** or **high-dimensional data**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison Table**\n",
        "| Regularization Type | Shrinks Coefficients | Sets Some Coefficients to Zero? | Best for |\n",
        "|---------------------|---------------------|-------------------------------|----------|\n",
        "| **Lasso (L1)**     | ‚úÖ Yes               | ‚úÖ Yes (Feature Selection)   | Sparse models (high-dimensional data) |\n",
        "| **Ridge (L2)**     | ‚úÖ Yes               | ‚ùå No                        | Multicollinearity (all features matter) |\n",
        "| **Elastic Net (L1+L2)** | ‚úÖ Yes        | ‚úÖ Yes (Some Zero, Some Small) | Combination of both |"
      ],
      "metadata": {
        "id": "KTlZtW6uCYkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge.\n",
        "\n",
        "ans. ### **When to Use Elastic Net Instead of Lasso or Ridge?**  \n",
        "\n",
        "Elastic Net is a **hybrid approach** that combines **Lasso (L1) and Ridge (L2) Regularization**. It is particularly useful in the following scenarios:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. When There Are Highly Correlated Features**  \n",
        "- **Lasso (L1) tends to randomly pick one feature and ignore others** if they are highly correlated.  \n",
        "- **Elastic Net distributes the coefficient weights across correlated features**, preventing loss of important information.\n",
        "\n",
        "‚úÖ **Use Elastic Net when your dataset has multicollinearity (highly correlated features).**  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. When You Have a High-Dimensional Dataset**  \n",
        "- If the number of features **(p)** is much greater than the number of samples **(n)** (e.g., **genomic data, text data**).  \n",
        "- Lasso can sometimes **over-select** and remove too many features, leading to **underfitting**.  \n",
        "- Elastic Net **balances feature selection and coefficient shrinkage** to maintain predictive power.\n",
        "\n",
        "‚úÖ **Use Elastic Net when dealing with high-dimensional data where feature selection is important but complete elimination of features is risky.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **3. When Lasso Struggles With Stability**  \n",
        "- If Lasso **eliminates too many features**, it can lead to an **unstable model** where small changes in data cause large changes in selected features.  \n",
        "- Elastic Net helps by **keeping small contributions from weakly related features**, reducing instability.\n",
        "\n",
        "‚úÖ **Use Elastic Net if Lasso is too aggressive in dropping features, leading to an unstable model.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **4. When You Need a Balance Between Lasso and Ridge**  \n",
        "- Ridge **does not perform feature selection**, while Lasso **performs aggressive feature selection**.  \n",
        "- Elastic Net provides a middle ground by allowing **some coefficients to be zero** while **shrinking others**.\n",
        "\n",
        "‚úÖ **Use Elastic Net when you want some feature selection but also want to retain some weak predictors.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **Summary: When to Use Which?**\n",
        "| **Regularization Type** | **Use When...** |\n",
        "|---------------------|----------------------------|\n",
        "| **Lasso (L1)** | You need **sparse models** (many irrelevant features) and **feature selection**. |\n",
        "| **Ridge (L2)** | You need **all features to contribute** and want to handle **multicollinearity**. |\n",
        "| **Elastic Net (L1 + L2)** | You have **correlated features** or **high-dimensional data**, and need a balance between Lasso and Ridge. |"
      ],
      "metadata": {
        "id": "V6NASAsuCjmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. C What is the impact of the regularization parameter (Œª) in Logistic Regression.\n",
        "\n",
        "ans. ### **Impact of the Regularization Parameter (Œª) in Logistic Regression**  \n",
        "\n",
        "The **regularization parameter (Œª)** controls the strength of regularization in Logistic Regression. It determines how much penalty is added to the cost function to prevent overfitting.  \n",
        "\n",
        "---\n",
        "\n",
        "### **1. Understanding Œª (Lambda)**\n",
        "- **High Œª (Strong Regularization):**  \n",
        "  - Shrinks the coefficients **closer to zero**.  \n",
        "  - Reduces model complexity ‚Üí **prevents overfitting**.  \n",
        "  - Can cause **underfitting** if too high.  \n",
        "\n",
        "- **Low Œª (Weak Regularization):**  \n",
        "  - Allows larger coefficient values.  \n",
        "  - Model becomes more flexible ‚Üí **can overfit**.  \n",
        "  - Works well when enough data is available.  \n",
        "\n",
        "- **Œª = 0 (No Regularization):**  \n",
        "  - Logistic Regression behaves like a **pure maximum likelihood model**.  \n",
        "  - Model is highly flexible but prone to **overfitting**.  \n",
        "\n",
        "> üîπ **Note:** In `sklearn`, the parameter **C** is the inverse of Œª:  \n",
        "  \\[\n",
        "  C = \\frac{1}{\\lambda}\n",
        "  \\]  \n",
        "  - **Higher C ‚Üí Less Regularization**  \n",
        "  - **Lower C ‚Üí More Regularization**  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Effect of Œª on Model Performance**  \n",
        "\n",
        "| **Œª (Regularization Strength)** | **Effect on Model** | **Effect on Coefficients** |\n",
        "|----------------------|----------------------|----------------------|\n",
        "| **Very Small (Close to 0)** | Overfits the data | Large coefficients |\n",
        "| **Optimal Œª** | Best trade-off between bias & variance | Medium-sized coefficients |\n",
        "| **Very Large (High Œª)** | Underfits the data | Coefficients shrink to nearly zero |\n",
        "\n",
        "‚úÖ **Choosing the right Œª is crucial** and is often done using **Cross-Validation** or **Grid Search (Hyperparameter Tuning)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Practical Impact in L1 (Lasso) vs. L2 (Ridge)**\n",
        "- **L1 Regularization (Lasso):**  \n",
        "  - Higher Œª ‚Üí **More coefficients become exactly zero** (feature selection).  \n",
        "  - Too high ‚Üí Can eliminate useful features.  \n",
        "\n",
        "- **L2 Regularization (Ridge):**  \n",
        "  - Higher Œª ‚Üí **Coefficients shrink but do not reach zero**.  \n",
        "  - Good for handling **multicollinearity**.  \n",
        "\n",
        "- **Elastic Net (L1 + L2):**  \n",
        "  - Balances **feature selection** (L1) and **coefficient shrinkage** (L2).  \n",
        "\n",
        "---\n",
        "\n",
        "### **4. How to Choose Œª (Tuning the Parameter)?**\n",
        "- Use **Cross-Validation (CV)** to find the best Œª.  \n",
        "- **GridSearchCV** or **RandomizedSearchCV** in `sklearn` can optimize Œª (C in `sklearn`).  \n",
        "- Common range:  \n",
        "  \\[\n",
        "  \\lambda \\in [10^{-4}, 10^4]\n",
        "  \\"
      ],
      "metadata": {
        "id": "JidgNFNtCwcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  What are the key assumptions of Logistic Regression.\n",
        "\n",
        "ans. ### **Key Assumptions of Logistic Regression**  \n",
        "\n",
        "Although Logistic Regression is widely used for classification tasks, it makes certain **assumptions** about the data. Violating these assumptions can impact the model's performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Dependent Variable is Binary (for Binary Classification)**\n",
        "- Logistic Regression is designed for **binary classification (0 or 1, Yes or No, Spam or Not Spam)**.  \n",
        "- For **multiclass classification**, extensions like **Softmax Regression (Multinomial Logistic Regression)** or **One-vs-Rest (OvR)** are used.  \n",
        "\n",
        "‚úÖ **Ensure the target variable is categorical and properly encoded (e.g., 0 & 1).**  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Independence of Observations (No Autocorrelation)**\n",
        "- Observations should be **independent** of each other.  \n",
        "- If there is a time dependency (e.g., stock prices, time series data), consider **Time Series Models (like LSTM, ARIMA)** instead.  \n",
        "\n",
        "‚úÖ **Check for autocorrelation if dealing with time-dependent data.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **3. Linearity of the Log-Odds**\n",
        "- Logistic Regression does **not assume linearity of the features with the output**, but it assumes that **log-odds (logit transformation) is linearly related to independent variables**.  \n",
        "- Mathematically:  \n",
        "  \\[\n",
        "  \\log \\left( \\frac{p}{1 - p} \\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n\n",
        "  \\]\n",
        "- If the relationship is non-linear, consider:\n",
        "  - **Transforming variables (log, square, polynomial features)**\n",
        "  - **Using non-linear models like Decision Trees or Neural Networks**  \n",
        "\n",
        "‚úÖ **Check for linearity using a logit transformation plot.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **4. No Multicollinearity (For Reliable Coefficients)**\n",
        "- Highly correlated independent variables **(multicollinearity)** can cause unstable coefficient estimates.  \n",
        "- Detect multicollinearity using:\n",
        "  - **Variance Inflation Factor (VIF)** ‚Üí If VIF > 10, drop or combine features.  \n",
        "  - **Correlation matrix** ‚Üí Remove highly correlated features.  \n",
        "- If multicollinearity is present, use **Ridge (L2) or Elastic Net Regularization**.  \n",
        "\n",
        "‚úÖ **Check for multicollinearity and apply regularization if needed.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **5. Sufficiently Large Sample Size**\n",
        "- Logistic Regression requires **a large enough dataset** to provide reliable estimates.  \n",
        "- If the dataset is too small, coefficients may be **unstable**, leading to **high variance**.  \n",
        "\n",
        "‚úÖ **Ensure sufficient sample size, especially for high-dimensional data.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **6. Absence of Outliers**\n",
        "- Logistic Regression is **sensitive to outliers**, as they can distort coefficient estimates.  \n",
        "- Detect outliers using:\n",
        "  - **Boxplots, Z-scores (>3 standard deviations), or IQR (Interquartile Range) Method**  \n",
        "  - **Robust regression techniques** (e.g., regularization)  \n",
        "\n",
        "‚úÖ **Identify and handle outliers to improve model stability.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **7. Balanced Class Distribution (for Better Performance)**\n",
        "- Logistic Regression performs poorly when **one class dominates** the dataset (class imbalance).  \n",
        "- Solutions:\n",
        "  - **Use weighted loss functions (`class_weight='balanced'` in `sklearn`)**\n",
        "  - **Resampling techniques** (Oversampling with SMOTE, Undersampling)  \n",
        "\n",
        "‚úÖ **Handle class imbalance for accurate classification results.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Logistic Regression Assumptions**\n",
        "| **Assumption** | **Description** | **Solution if Violated** |\n",
        "|--------------|-----------------|--------------------------|\n",
        "| **Binary Dependent Variable** | Target must be categorical (0/1). | Use multinomial logistic regression for multiclass. |\n",
        "| **Independence of Observations** | No correlation between samples. | Use Time Series models for dependent data. |\n",
        "| **Linearity of Log-Odds** | Log-odds must have a linear relationship with features. | Use polynomial/log transformations or non-linear models. |\n",
        "| **No Multicollinearity** | Independent variables should not be highly correlated. | Check **VIF** & apply **Ridge or Elastic Net Regularization**. |\n",
        "| **Sufficient Sample Size** | More samples ‚Üí better generalization. | Collect more data or use simpler models. |\n",
        "| **No Outliers** | Outliers can distort predictions. | Detect with IQR, Boxplots & remove/transform them. |\n",
        "| **Balanced Class Distribution** | Imbalanced data can mislead predictions. | Use **class weights, resampling, or ensemble methods**. |"
      ],
      "metadata": {
        "id": "TvTD6pk9C-dm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  What are some alternatives to Logistic Regression for classification tasks.\n",
        "\n",
        "ans. ### **Alternatives to Logistic Regression for Classification Tasks**  \n",
        "\n",
        "Logistic Regression is a great **baseline model** for classification, but it has limitations, especially with **non-linearity, high-dimensional data, and complex relationships**. Below are some alternative models for classification:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Decision Trees üå≥**\n",
        "‚úÖ **Best When:**  \n",
        "- Data has **non-linear relationships**.  \n",
        "- Model interpretability is important (easy to visualize).  \n",
        "- You need a fast model that works on smaller datasets.  \n",
        "\n",
        "‚ùå **Limitations:**  \n",
        "- Prone to **overfitting** (can be solved using pruning or ensembles).  \n",
        "\n",
        "üîπ **Python Library:** `sklearn.tree.DecisionTreeClassifier`  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Random Forest üå≤üå≤üå≤ (Ensemble of Decision Trees)**\n",
        "‚úÖ **Best When:**  \n",
        "- You need **high accuracy and robustness**.  \n",
        "- You want to **reduce overfitting** compared to a single Decision Tree.  \n",
        "- You need a **feature importance ranking**.  \n",
        "\n",
        "‚ùå **Limitations:**  \n",
        "- Computationally expensive for very large datasets.  \n",
        "\n",
        "üîπ **Python Library:** `sklearn.ensemble.RandomForestClassifier`  \n",
        "\n",
        "---\n",
        "\n",
        "### **3. Gradient Boosting (GBM, XGBoost, LightGBM, CatBoost) üöÄ**\n",
        "‚úÖ **Best When:**  \n",
        "- You need **the highest accuracy possible**.  \n",
        "- Your data has **complex patterns and non-linearity**.  \n",
        "- You have a large dataset and computational power.  \n",
        "\n",
        "‚ùå **Limitations:**  \n",
        "- **Slow training time** (especially on large datasets).  \n",
        "- Requires **hyperparameter tuning** for best performance.  \n",
        "\n",
        "üîπ **Popular Boosting Libraries:**  \n",
        "- `sklearn.ensemble.GradientBoostingClassifier`  \n",
        "- `xgboost.XGBClassifier`  \n",
        "- `lightgbm.LGBMClassifier`  \n",
        "- `catboost.CatBoostClassifier`  \n",
        "\n",
        "---\n",
        "\n",
        "### **4. Support Vector Machine (SVM) üèÜ**\n",
        "‚úÖ **Best When:**  \n",
        "- You have **small to medium datasets**.  \n",
        "- Data is **not linearly separable** (use **Kernel Trick**).  \n",
        "- You want a **robust model that handles outliers well**.  \n",
        "\n",
        "‚ùå **Limitations:**  \n",
        "- **Computationally expensive** for large datasets.  \n",
        "- **Hard to tune hyperparameters** like `C` and `gamma`.  \n",
        "\n",
        "üîπ **Python Library:** `sklearn.svm.SVC`  \n",
        "\n",
        "---\n",
        "\n",
        "### **5. k-Nearest Neighbors (k-NN) üë•**\n",
        "‚úÖ **Best When:**  \n",
        "- You need a **simple, non-parametric** model.  \n",
        "- You want to classify based on **similarity (distance-based method)**.  \n",
        "- The dataset is **small and noise-free**.  \n",
        "\n",
        "‚ùå **Limitations:**  \n",
        "- **Computationally expensive** for large datasets.  \n",
        "- **Sensitive to noise** (no feature selection).  \n",
        "\n",
        "üîπ **Python Library:** `sklearn.neighbors.KNeighborsClassifier`  \n",
        "\n",
        "---\n",
        "\n",
        "### **6. Na√Øve Bayes ü§ñ**\n",
        "‚úÖ **Best When:**  \n",
        "- Features are **conditionally independent**.  \n",
        "- You have **text data (e.g., spam detection, sentiment analysis)**.  \n",
        "- You need a **fast and simple probabilistic model**.  \n",
        "\n",
        "‚ùå **Limitations:**  \n",
        "- Assumes **feature independence**, which is rarely true in real-world data.  \n",
        "\n",
        "üîπ **Types of Na√Øve Bayes:**  \n",
        "- **GaussianNB (for continuous data)**  \n",
        "- **MultinomialNB (for text data)**  \n",
        "- **BernoulliNB (for binary features)**  \n",
        "\n",
        "üîπ **Python Library:** `sklearn.naive_bayes.GaussianNB`  \n",
        "\n",
        "---\n",
        "\n",
        "### **7. Artificial Neural Networks (ANN) üß†**\n",
        "‚úÖ **Best When:**  \n",
        "- You have **large datasets** with complex patterns.  \n",
        "- You need a model that can **learn highly non-linear relationships**.  \n",
        "- You want to apply **Deep Learning for classification**.  \n",
        "\n",
        "‚ùå **Limitations:**  \n",
        "- Requires **high computational power** (especially deep networks).  \n",
        "- Needs a **lot of data to generalize well**.  \n",
        "\n",
        "üîπ **Python Library:** `tensorflow` or `pytorch`  \n",
        "\n",
        "---\n",
        "\n",
        "### **8. Bayesian Logistic Regression üìä**\n",
        "‚úÖ **Best When:**  \n",
        "- You need **uncertainty estimation** for predictions.  \n",
        "- You want a **Bayesian approach to classification**.  \n",
        "\n",
        "‚ùå **Limitations:**  \n",
        "- Computationally expensive for large datasets.  \n",
        "\n",
        "üîπ **Python Library:** `pymc3` or `statsmodels`  \n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison Table**\n",
        "| **Algorithm** | **Handles Non-Linearity?** | **Works on Large Data?** | **Interpretable?** | **Main Limitation** |\n",
        "|--------------|-----------------|-----------------|-----------------|----------------|\n",
        "| **Logistic Regression** | ‚ùå No | ‚úÖ Yes | ‚úÖ High | Assumes linearity |\n",
        "| **Decision Trees** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ High | Overfitting |\n",
        "| **Random Forest** | ‚úÖ Yes | ‚úÖ Yes | üü° Medium | Slow for large data |\n",
        "| **Gradient Boosting (XGBoost, LightGBM)** | ‚úÖ Yes | ‚úÖ Yes | üü° Medium | Complex tuning |\n",
        "| **SVM** | ‚úÖ Yes (with kernel) | ‚ùå No | üü° Medium | Slow for large data |\n",
        "| **k-NN** | ‚úÖ Yes | ‚ùå No | ‚úÖ High | Slow, sensitive to noise |\n",
        "| **Na√Øve Bayes** | ‚ùå No | ‚úÖ Yes | ‚úÖ High | Feature independence assumption |\n",
        "| **Neural Networks** | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No | Requires a lot of data |\n",
        "| **Bayesian Logistic Regression** | ‚ùå No | ‚ùå No | ‚úÖ High | Computationally expensive |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion: Which Alternative to Choose?**\n",
        "- **If you need interpretability ‚Üí** **Logistic Regression, Decision Trees, Na√Øve Bayes**  \n",
        "- **If you need accuracy ‚Üí** **Gradient Boosting (XGBoost, LightGBM), Random Forest**  \n",
        "- **If you have small data ‚Üí** **SVM, k-NN, Na√Øve Bayes**  \n",
        "- **If you have big data & deep patterns ‚Üí** **Neural Networks**  "
      ],
      "metadata": {
        "id": "DO7yKg5-DOJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What are Classification Evaluation Metrics.\n",
        "\n",
        "ans. ### **Classification Evaluation Metrics** üìä  \n",
        "\n",
        "When evaluating a classification model (like Logistic Regression), we use different metrics to measure **how well the model performs**. The right metric depends on the problem, especially if there is **class imbalance**.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Accuracy** ‚úÖ\n",
        "- Measures **overall correctness** of the model.  \n",
        "- Formula:  \n",
        "  \\[\n",
        "  Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "  \\]\n",
        "- Works **well for balanced datasets**, but **fails for imbalanced data**.  \n",
        "\n",
        "üîπ **Example:** If a dataset has **95% Class A** and **5% Class B**, predicting all as Class A gives **95% accuracy**, but the model is useless!  \n",
        "\n",
        "‚úÖ **Use Accuracy when:** Classes are **balanced**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Precision (Positive Predictive Value - PPV) üéØ**\n",
        "- Measures **how many predicted positives are actually correct**.  \n",
        "- Formula:  \n",
        "  \\[\n",
        "  Precision = \\frac{TP}{TP + FP}\n",
        "  \\]\n",
        "- High Precision = **Fewer False Positives (FPs)**  \n",
        "\n",
        "‚úÖ **Use Precision when:**  \n",
        "- **False Positives are costly** (e.g., predicting a disease when it's not present).  \n",
        "\n",
        "üîπ **Example:** In **spam detection**, a False Positive means an important email is classified as spam. We want **high Precision** to avoid this.  \n",
        "\n",
        "---\n",
        "\n",
        "## **3. Recall (Sensitivity, True Positive Rate - TPR) üì¢**\n",
        "- Measures **how many actual positives were correctly predicted**.  \n",
        "- Formula:  \n",
        "  \\[\n",
        "  Recall = \\frac{TP}{TP + FN}\n",
        "  \\]\n",
        "- High Recall = **Fewer False Negatives (FNs)**  \n",
        "\n",
        "‚úÖ **Use Recall when:**  \n",
        "- **False Negatives are costly** (e.g., detecting cancer, fraud detection).  \n",
        "\n",
        "üîπ **Example:** In **cancer detection**, a False Negative means a sick person is diagnosed as healthy, which is very dangerous. We want **high Recall**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **4. F1-Score (Harmonic Mean of Precision & Recall) ‚öñÔ∏è**\n",
        "- Balances **Precision & Recall**.  \n",
        "- Formula:  \n",
        "  \\[\n",
        "  F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
        "  \\]\n",
        "- **Best when data is imbalanced**.  \n",
        "- F1-score ranges from **0 (worst) to 1 (best)**.  \n",
        "\n",
        "‚úÖ **Use F1-Score when:**  \n",
        "- Both **False Positives and False Negatives are important**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **5. ROC Curve (Receiver Operating Characteristic) & AUC üé¢**\n",
        "- ROC Curve plots **True Positive Rate (Recall) vs. False Positive Rate (FPR)**.  \n",
        "- **AUC (Area Under Curve):**  \n",
        "  - **AUC = 1** ‚Üí Perfect model.  \n",
        "  - **AUC = 0.5** ‚Üí Random guessing.  \n",
        "  - **Higher AUC = Better classification power**.  \n",
        "\n",
        "‚úÖ **Use ROC-AUC when:**  \n",
        "- You need **overall model performance comparison**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **6. Log Loss (Logarithmic Loss) üìâ**\n",
        "- Measures how confident the model is in its predictions.  \n",
        "- Formula:  \n",
        "  \\[\n",
        "  LogLoss = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
        "  \\]\n",
        "- **Lower Log Loss = Better model performance**.  \n",
        "\n",
        "‚úÖ **Use Log Loss when:**  \n",
        "- You need a **probabilistic confidence measure** for predictions.  \n",
        "\n",
        "---\n",
        "\n",
        "## **7. Matthews Correlation Coefficient (MCC) üìä**\n",
        "- A more balanced metric for **imbalanced datasets**.  \n",
        "- Formula:  \n",
        "  \\[\n",
        "  MCC = \\frac{(TP \\times TN) - (FP \\times FN)}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n",
        "  \\]\n",
        "- Ranges from **-1 (worst) to 1 (best)**, with **0 meaning random guessing**.  \n",
        "\n",
        "‚úÖ **Use MCC when:**  \n",
        "- You need a **single number summary for imbalanced data**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **8. Cohen‚Äôs Kappa üèÜ**\n",
        "- Measures model performance compared to **random chance**.  \n",
        "- Formula:  \n",
        "  \\[\n",
        "  Kappa = \\frac{p_o - p_e}{1 - p_e}\n",
        "  \\]\n",
        "  - \\( p_o \\) = observed accuracy  \n",
        "  - \\( p_e \\) = expected accuracy (by chance)  \n",
        "- **Kappa = 1** ‚Üí Perfect agreement, **Kappa = 0** ‚Üí Random guessing.  \n",
        "\n",
        "‚úÖ **Use Cohen‚Äôs Kappa when:**  \n",
        "- You need a **robust accuracy measure** that accounts for chance agreement.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Metrics**\n",
        "| **Metric** | **Best For** | **When to Use** |\n",
        "|-----------|-------------|----------------|\n",
        "| **Accuracy** | General performance | Only for **balanced** data |\n",
        "| **Precision** | Minimizing False Positives | Spam detection, fraud detection |\n",
        "| **Recall** | Minimizing False Negatives | Medical diagnosis, fraud detection |\n",
        "| **F1-Score** | Balance of Precision & Recall | When both FP & FN matter |\n",
        "| **ROC-AUC** | Model ranking power | When you need overall model evaluation |\n",
        "| **Log Loss** | Probabilistic confidence | When probability predictions matter |\n",
        "| **MCC** | Imbalanced datasets | When a single metric is needed |\n",
        "| **Cohen‚Äôs Kappa** | Adjusted accuracy | When chance agreement matters |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "n270IuDMDf9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  How does class imbalance affect Logistic Regression.\n",
        "\n",
        "ans. ### **How Does Class Imbalance Affect Logistic Regression?**  \n",
        "\n",
        "Class imbalance occurs when one class is significantly more frequent than the other(s). This can negatively impact **Logistic Regression**, leading to **biased predictions** and **poor model performance**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Problems Caused by Class Imbalance in Logistic Regression**  \n",
        "\n",
        "#### **1. Biased Model Predictions üèÜ**\n",
        "- Logistic Regression **minimizes overall error**.\n",
        "- If **95%** of data belongs to Class A and **5%** to Class B, the model may **predict everything as Class A** to achieve **high accuracy (~95%)**, even though it completely **fails to detect Class B**.  \n",
        "\n",
        "‚úÖ **Example:**  \n",
        "- Predicting fraud cases in bank transactions.\n",
        "- Fraudulent cases (1%) vs. Non-fraudulent (99%).\n",
        "- Model predicts **\"No Fraud\" 100% of the time**, leading to **high accuracy but poor recall**.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Poor Recall for the Minority Class (False Negatives üö®)**\n",
        "- The model **fails to detect the minority class** (e.g., fraud, disease detection).\n",
        "- High **False Negatives (FN)** ‚Üí **Serious consequences** in medical or security applications.\n",
        "\n",
        "‚úÖ **Example:**  \n",
        "- Cancer detection: If the model **misclassifies cancer patients as healthy**, it can be **life-threatening**.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Misleading Accuracy (Accuracy Paradox) ü§î**\n",
        "- Accuracy **alone is not a good metric** for imbalanced data.\n",
        "- Even if the model **predicts all cases as the majority class**, it can still show **high accuracy**.\n",
        "\n",
        "‚úÖ **Example:**  \n",
        "- **99% accuracy sounds great**, but if the **minority class (e.g., fraud) is never detected**, the model is useless.  \n",
        "- **F1-Score, Precision-Recall, and ROC-AUC are better metrics**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Solutions to Handle Class Imbalance in Logistic Regression**  \n",
        "\n",
        "#### **1. Use Better Evaluation Metrics üèÜ**\n",
        "- **Avoid Accuracy!** Instead, use:\n",
        "  - **Precision & Recall**\n",
        "  - **F1-Score**\n",
        "  - **ROC-AUC**\n",
        "  - **Matthews Correlation Coefficient (MCC)**\n",
        "  - **Precision-Recall Curve**  \n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Resampling Techniques (Fixing the Dataset) üìä**  \n",
        "‚úÖ **Oversampling (SMOTE - Synthetic Minority Over-sampling Technique)**  \n",
        "- **Duplicates** or **creates synthetic samples** of the minority class to balance data.  \n",
        "- Works well for **small datasets**.  \n",
        "\n",
        "‚úÖ **Undersampling (Random Undersampling)**\n",
        "- **Removes** samples from the majority class to balance data.  \n",
        "- Risk: Losing valuable information.  \n",
        "\n",
        "‚úÖ **Hybrid Method (SMOTE + Tomek Links)**\n",
        "- Combination of **oversampling and undersampling** to maintain balance.  \n",
        "\n",
        "üîπ **Python Example (SMOTE):**  \n",
        "```python\n",
        "from imblearn.over_sampling import SMOTE\n",
        "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Adjust Class Weights in Logistic Regression ‚öñÔ∏è**  \n",
        "- Use **class_weight='balanced'** in `LogisticRegression()`  \n",
        "- Gives **higher penalty** to the minority class.  \n",
        "\n",
        "üîπ **Python Example:**  \n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Adjust Decision Threshold (Probability Cutoff) üìâ**  \n",
        "- Default **Logistic Regression uses 0.5 as a cutoff** (if probability > 0.5 ‚Üí Class 1).  \n",
        "- For imbalanced datasets, **lower the threshold** to detect more of the minority class.  \n",
        "\n",
        "üîπ **Python Example:**  \n",
        "```python\n",
        "y_probs = model.predict_proba(X_test)[:,1]\n",
        "new_threshold = 0.3  # Lower than 0.5\n",
        "y_pred = (y_probs > new_threshold).astype(int)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Use Alternative Algorithms üèÜ**\n",
        "- Some algorithms handle class imbalance **better than Logistic Regression**:\n",
        "  - **Random Forest (with class_weight)**  \n",
        "  - **XGBoost (scale_pos_weight parameter)**  \n",
        "  - **Balanced Bagging Classifier**  \n",
        "\n",
        "üîπ **Python Example (XGBoost with class weight):**  \n",
        "```python\n",
        "import xgboost as xgb\n",
        "model = xgb.XGBClassifier(scale_pos_weight=10)  # Adjust weight based on class ratio\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Thoughts: What Should You Do?**\n",
        "| **Method** | **When to Use?** |\n",
        "|------------|----------------|\n",
        "| **Class Weights (`class_weight='balanced'`)** | If you want a simple fix in Logistic Regression |\n",
        "| **SMOTE (Oversampling)** | If you have **few minority samples** (avoid overfitting) |\n",
        "| **Undersampling** | If you have **too much majority class data** |\n",
        "| **Threshold Tuning** | If **Recall is more important than Precision** |\n",
        "| **Alternative Models (Random Forest, XGBoost, etc.)** | If you want **better performance** |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion:**\n",
        "‚úÖ Class imbalance can make Logistic Regression **biased** towards the majority class.  \n",
        "‚úÖ Using **resampling, class weights, and better evaluation metrics** can improve model performance.  \n",
        "‚úÖ **Avoid accuracy** as a metric‚Äîuse **Precision, Recall, and F1-Score instead**.  \n",
        "‚úÖ Consider **alternative models like XGBoost or Random Forest** for better handling of imbalance.  "
      ],
      "metadata": {
        "id": "NfaOeDPMDyBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What is Hyperparameter Tuning in Logistic Regression.\n",
        "\n",
        "ans. ### **Hyperparameter Tuning in Logistic Regression** üöÄ  \n",
        "\n",
        "**Hyperparameter tuning** is the process of **optimizing** the hyperparameters of a machine learning model to improve performance. Unlike model parameters (e.g., weights in Logistic Regression), **hyperparameters are set before training** and control how the model learns.\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Hyperparameters in Logistic Regression**  \n",
        "\n",
        "1Ô∏è‚É£ **C (Inverse of Regularization Strength)**  \n",
        "   - Controls how much the model is **penalized for large coefficients**.  \n",
        "   - **Higher C** ‚Üí Less regularization ‚Üí More complex model.  \n",
        "   - **Lower C** ‚Üí More regularization ‚Üí Simpler model, avoids overfitting.  \n",
        "\n",
        "2Ô∏è‚É£ **Penalty (Regularization Type)**  \n",
        "   - `\"l1\"` ‚Üí Lasso Regression (Feature Selection).  \n",
        "   - `\"l2\"` ‚Üí Ridge Regression (Prevents large coefficients).  \n",
        "   - `\"elasticnet\"` ‚Üí Combination of Lasso & Ridge.  \n",
        "\n",
        "3Ô∏è‚É£ **Solver (Optimization Algorithm)**  \n",
        "   - **Common solvers**:  \n",
        "     - `\"liblinear\"` (Best for small datasets, supports L1 & L2).  \n",
        "     - `\"lbfgs\"` (Handles large datasets, supports L2 & Multiclass).  \n",
        "     - `\"saga\"` (Works with L1, L2, and Elastic Net).  \n",
        "\n",
        "4Ô∏è‚É£ **Max Iterations (`max_iter`)**  \n",
        "   - Controls how many times the optimization algorithm runs before stopping.  \n",
        "   - **Higher values** help with convergence issues but take more time.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Methods for Hyperparameter Tuning** üéØ  \n",
        "\n",
        "### **1. GridSearchCV (Exhaustive Search) üîç**\n",
        "- Tests **all possible combinations** of hyperparameters.  \n",
        "- Finds the **best combination** using cross-validation.  \n",
        "\n",
        "üîπ **Python Code Example:**  \n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],  \n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. RandomizedSearchCV (Faster Alternative) ‚ö°**\n",
        "- Instead of testing **all** combinations, it **randomly selects** a subset.  \n",
        "- Faster for **large hyperparameter spaces**.  \n",
        "\n",
        "üîπ **Python Code Example:**  \n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "param_dist = {\n",
        "    'C': uniform(0.01, 100),  \n",
        "    'penalty': ['l1', 'l2'],  \n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(log_reg, param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Bayesian Optimization (More Efficient) üß†**\n",
        "- Uses probabilistic models to **choose promising hyperparameter combinations**.\n",
        "- More efficient than GridSearch and RandomizedSearch.  \n",
        "\n",
        "üîπ **Best for:** Expensive models where testing all hyperparameters is **not feasible**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Which Method Should You Use?**\n",
        "| **Method** | **When to Use?** |\n",
        "|------------|----------------|\n",
        "| **GridSearchCV** | Small datasets, you want the best combination |\n",
        "| **RandomizedSearchCV** | Large datasets, faster than GridSearch |\n",
        "| **Bayesian Optimization** | Complex models, very large hyperparameter space |\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Thoughts**\n",
        "‚úÖ **Hyperparameter tuning helps improve Logistic Regression performance**.  \n",
        "‚úÖ **GridSearchCV is thorough but slow**, while **RandomizedSearchCV is faster**.  \n",
        "‚úÖ **Regularization (`C` and `penalty`) is critical for avoiding overfitting**.  \n",
        "‚úÖ **Solver choice depends on dataset size and type of regularization**.  "
      ],
      "metadata": {
        "id": "c1Bys_JREKpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  What are different solvers in Logistic Regression? Which one should be used.\n",
        "\n",
        "ans. ### **Different Solvers in Logistic Regression & When to Use Them** üöÄ  \n",
        "\n",
        "In **Scikit-Learn‚Äôs `LogisticRegression`**, the `solver` parameter determines which **optimization algorithm** is used to minimize the **cost function**. Different solvers perform better for different dataset sizes and types of regularization.\n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ List of Solvers in Logistic Regression**  \n",
        "\n",
        "| **Solver** | **Supports L1?** | **Supports L2?** | **Supports Elastic Net?** | **Best For** |\n",
        "|------------|----------------|----------------|--------------------|------------|\n",
        "| **liblinear** | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No | Small datasets, sparse data |\n",
        "| **lbfgs** | ‚ùå No | ‚úÖ Yes | ‚ùå No | Large datasets, multiclass problems |\n",
        "| **newton-cg** | ‚ùå No | ‚úÖ Yes | ‚ùå No | Large datasets, faster convergence |\n",
        "| **sag** | ‚ùå No | ‚úÖ Yes | ‚ùå No | Very large datasets, online learning |\n",
        "| **saga** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | Large datasets, L1, L2, Elastic Net |\n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Choosing the Right Solver** üéØ  \n",
        "\n",
        "| **Scenario** | **Recommended Solver** |\n",
        "|-------------|----------------------|\n",
        "| **Small dataset** (‚â§ 10,000 samples) | `liblinear` |\n",
        "| **Large dataset** (> 10,000 samples) | `lbfgs` or `sag` |\n",
        "| **Sparse data (many zeros)** | `liblinear` or `saga` |\n",
        "| **Multiclass classification (`multi_class='multinomial'`)** | `lbfgs`, `newton-cg`, `sag` |\n",
        "| **L1 Regularization (Lasso)** | `liblinear` or `saga` |\n",
        "| **L2 Regularization (Ridge)** | Any solver (except `saga` for large datasets) |\n",
        "| **Elastic Net Regularization** | `saga` |\n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Solver-Specific Details**  \n",
        "\n",
        "### **üîπ `liblinear` (Best for Small Datasets)**\n",
        "- Uses **Coordinate Descent (CD)** optimization.\n",
        "- Works well with **small datasets** and **L1/L2 regularization**.\n",
        "- **Not suitable for large datasets** due to **slower performance**.\n",
        "\n",
        "**Use when:**\n",
        "‚úÖ Small dataset  \n",
        "‚úÖ L1 or L2 regularization  \n",
        "‚úÖ Sparse dataset  \n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ `lbfgs` (Best for Large Datasets & Multiclass)**\n",
        "- Uses **BFGS (Broyden-Fletcher-Goldfarb-Shanno) approximation**.\n",
        "- Supports **only L2 regularization**.\n",
        "- Works well for **large datasets** and **multiclass problems** (`multi_class='multinomial'`).\n",
        "\n",
        "**Use when:**\n",
        "‚úÖ Large dataset  \n",
        "‚úÖ Multiclass classification  \n",
        "‚úÖ Only L2 regularization  \n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ `newton-cg` (Faster Convergence for Large Data)**\n",
        "- Uses **Newton‚Äôs method with conjugate gradient**.\n",
        "- Suitable for **L2 regularization**.\n",
        "- More **memory efficient** than `lbfgs` for large datasets.\n",
        "\n",
        "**Use when:**\n",
        "‚úÖ Large dataset  \n",
        "‚úÖ Multiclass classification  \n",
        "‚úÖ Faster than `lbfgs` for some cases  \n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ `sag` (Best for Very Large Datasets & Online Learning)**\n",
        "- Uses **Stochastic Average Gradient (SAG) Descent**.\n",
        "- Works well with **very large datasets**.\n",
        "- Only supports **L2 regularization**.\n",
        "\n",
        "**Use when:**\n",
        "‚úÖ Extremely large datasets (> 100,000 samples)  \n",
        "‚úÖ Online learning (updates weights dynamically)  \n",
        "‚úÖ Only L2 regularization  \n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ `saga` (Best for Large Data + L1, L2, & Elastic Net)**\n",
        "- Variation of `sag` that supports **L1, L2, and Elastic Net regularization**.\n",
        "- Best for **sparse datasets** (lots of zeros).\n",
        "\n",
        "**Use when:**\n",
        "‚úÖ Large dataset  \n",
        "‚úÖ Need L1, L2, or Elastic Net regularization  \n",
        "‚úÖ Sparse dataset  \n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Which Solver Should You Use? (Summary)**\n",
        "üîπ **Small datasets ‚Üí `liblinear`**  \n",
        "üîπ **Large datasets ‚Üí `lbfgs`, `newton-cg`, or `sag`**  \n",
        "üîπ **Sparse datasets ‚Üí `liblinear` or `saga`**  \n",
        "üîπ **L1 regularization ‚Üí `liblinear` or `saga`**  \n",
        "üîπ **Elastic Net ‚Üí `saga`**  \n",
        "üîπ **Multiclass classification ‚Üí `lbfgs`, `newton-cg`, or `sag`**  \n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ Example: Choosing a Solver in Python**\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Small dataset with L1 regularization\n",
        "model1 = LogisticRegression(solver='liblinear', penalty='l1')\n",
        "\n",
        "# Large dataset with L2 regularization\n",
        "model2 = LogisticRegression(solver='lbfgs', penalty='l2')\n",
        "\n",
        "# Sparse dataset with Elastic Net regularization\n",
        "model3 = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.5)\n",
        "```"
      ],
      "metadata": {
        "id": "t0KFssADEaeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification.\n",
        "\n",
        "ans. ## **Logistic Regression for Multiclass Classification** üéØ  \n",
        "\n",
        "Logistic Regression is primarily used for **binary classification** (0 or 1). However, it can be extended for **multiclass classification** using two main approaches:  \n",
        "\n",
        "1Ô∏è‚É£ **One-vs-Rest (OvR) Strategy**  \n",
        "2Ô∏è‚É£ **Softmax (Multinomial) Regression**  \n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ One-vs-Rest (OvR) Strategy (Default in Scikit-Learn)**  \n",
        "\n",
        "- Also called **One-vs-All (OvA)**.  \n",
        "- Trains **one binary classifier per class** against all other classes.  \n",
        "- For a dataset with **N classes**, it trains **N classifiers**.  \n",
        "- During prediction, the classifier with the **highest probability** wins.  \n",
        "\n",
        "### **Example of OvR**\n",
        "| Class | Model Training |\n",
        "|-------|--------------|\n",
        "| Class 1 vs (Class 2 + Class 3) | Logistic Regression 1 |\n",
        "| Class 2 vs (Class 1 + Class 3) | Logistic Regression 2 |\n",
        "| Class 3 vs (Class 1 + Class 2) | Logistic Regression 3 |\n",
        "\n",
        "üìå **Pros:**  \n",
        "‚úÖ Works well for small datasets.  \n",
        "‚úÖ Computationally efficient.  \n",
        "‚úÖ Simple and interpretable.  \n",
        "\n",
        "üìå **Cons:**  \n",
        "‚ùå Can be **biased if classes are imbalanced**.  \n",
        "‚ùå **Overlapping decision boundaries** might reduce accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Softmax (Multinomial) Regression**  \n",
        "\n",
        "- Uses **Softmax function** to calculate probabilities across all classes at once.  \n",
        "- Trains **a single model** for all classes.  \n",
        "- The model predicts the class with the **highest probability**.  \n",
        "- The weight matrix has **N classes and M features**, instead of **N separate models**.  \n",
        "\n",
        "### **Softmax Function Formula** üìè  \n",
        "For class \\( j \\), the probability is:\n",
        "\n",
        "\\[\n",
        "P(y = j | x) = \\frac{e^{W_j \\cdot X + b_j}}{\\sum_{k=1}^{N} e^{W_k \\cdot X + b_k}}\n",
        "\\]\n",
        "\n",
        "where:  \n",
        "- \\( W_j \\) = weight vector for class \\( j \\).  \n",
        "- \\( X \\) = input features.  \n",
        "- \\( b_j \\) = bias for class \\( j \\).  \n",
        "- \\( N \\) = number of classes.  \n",
        "\n",
        "üìå **Pros:**  \n",
        "‚úÖ Works well for **large datasets**.  \n",
        "‚úÖ **More accurate** compared to OvR for **balanced datasets**.  \n",
        "‚úÖ Produces **probabilities across all classes**.  \n",
        "\n",
        "üìå **Cons:**  \n",
        "‚ùå **Computationally expensive** for large datasets.  \n",
        "‚ùå **Slower training** than OvR.  \n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ How to Implement in Scikit-Learn**  \n",
        "\n",
        "### **üëâ Using One-vs-Rest (OvR)**\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train Logistic Regression with OvR (default)\n",
        "model_ovr = LogisticRegression(multi_class='ovr', solver='liblinear')  \n",
        "model_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model_ovr.predict(X_test)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üëâ Using Softmax (Multinomial)**\n",
        "```python\n",
        "# Train Logistic Regression with Softmax\n",
        "model_softmax = LogisticRegression(multi_class='multinomial', solver='lbfgs')  \n",
        "model_softmax.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model_softmax.predict(X_test)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ When to Use OvR vs Softmax?**  \n",
        "\n",
        "| **Scenario** | **Recommended Approach** |\n",
        "|-------------|----------------------|\n",
        "| Small dataset | **OvR** |\n",
        "| Large dataset | **Softmax** |\n",
        "| Imbalanced dataset | **OvR** (better for rare classes) |\n",
        "| Computational efficiency required | **OvR** (faster training) |\n",
        "| High accuracy needed | **Softmax** (better probability estimates) |\n",
        "\n",
        "---\n",
        "\n",
        "## **5Ô∏è‚É£ Summary**  \n",
        "‚úÖ **OvR** trains **multiple binary classifiers**, while **Softmax** trains **one model**.  \n",
        "‚úÖ **OvR is simpler & faster**, while **Softmax is more accurate** for large datasets.  \n",
        "‚úÖ Use **Softmax if you need probability estimates across all classes**.  \n",
        "‚úÖ Use **OvR for small or imbalanced datasets**.  "
      ],
      "metadata": {
        "id": "8CsZGyhtEsEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What are the advantages and disadvantages of Logistic Regression.\n",
        "\n",
        "ans. ## **Advantages & Disadvantages of Logistic Regression** üéØ  \n",
        "\n",
        "Logistic Regression is a simple yet powerful algorithm used for **classification tasks**. However, it has its **strengths and limitations** compared to other models.\n",
        "\n",
        "---\n",
        "\n",
        "## **‚úÖ Advantages of Logistic Regression**\n",
        "### **1Ô∏è‚É£ Simple and Easy to Interpret**\n",
        "- Logistic Regression is **easy to implement** and interpret.\n",
        "- The **coefficients** provide insights into feature importance.\n",
        "\n",
        "### **2Ô∏è‚É£ Works Well for Linearly Separable Data**\n",
        "- Performs well when classes are **separated by a straight line (or hyperplane in higher dimensions).**\n",
        "\n",
        "### **3Ô∏è‚É£ Fast and Efficient**\n",
        "- **Computationally inexpensive**, especially for small to medium-sized datasets.\n",
        "- **Requires less training time** compared to deep learning models.\n",
        "\n",
        "### **4Ô∏è‚É£ Provides Probability Scores**\n",
        "- Predicts probabilities using the **Sigmoid function**.\n",
        "- Useful for **risk analysis, medical diagnosis, and fraud detection**.\n",
        "\n",
        "### **5Ô∏è‚É£ Robust to Small Data**\n",
        "- Works well even with **small datasets**, unlike neural networks which require large data.\n",
        "\n",
        "### **6Ô∏è‚É£ Can Handle Multiclass Classification**\n",
        "- Can be extended to **multiclass problems** using **One-vs-Rest (OvR) or Softmax (Multinomial) Regression**.\n",
        "\n",
        "### **7Ô∏è‚É£ Can Be Regularized**\n",
        "- Supports **L1 (Lasso), L2 (Ridge), and Elastic Net** regularization to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "## **‚ùå Disadvantages of Logistic Regression**\n",
        "### **1Ô∏è‚É£ Assumes a Linear Decision Boundary**\n",
        "- Does not work well if the data is **non-linearly separable**.\n",
        "- **Solution:** Use a **non-linear model** (e.g., Decision Trees, SVM with kernels, Neural Networks).\n",
        "\n",
        "### **2Ô∏è‚É£ Sensitive to Outliers**\n",
        "- **Outliers** can significantly impact model performance.\n",
        "- **Solution:** Use **Robust Scaling (e.g., IQR, Median Scaling)** or **remove outliers**.\n",
        "\n",
        "### **3Ô∏è‚É£ Cannot Handle Highly Correlated Features**\n",
        "- **Multicollinearity** affects model interpretation and coefficients.\n",
        "- **Solution:** Use **feature selection** or **Principal Component Analysis (PCA)**.\n",
        "\n",
        "### **4Ô∏è‚É£ Requires Large Sample Size for Complex Data**\n",
        "- Struggles when the dataset has **too many features** (high-dimensional space).\n",
        "- **Solution:** Use **feature engineering** or **Deep Learning models**.\n",
        "\n",
        "### **5Ô∏è‚É£ Poor Performance with Imbalanced Data**\n",
        "- If one class dominates, the model predicts the majority class most of the time.\n",
        "- **Solution:** Use **SMOTE (Synthetic Minority Over-sampling Technique)** or **class weighting**.\n",
        "\n",
        "### **6Ô∏è‚É£ Assumes Independent Features**\n",
        "- Assumes **no correlation** between independent variables.\n",
        "- **Solution:** Use **feature selection techniques**.\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ When to Use Logistic Regression?**\n",
        "‚úÖ When the data is **linearly separable**  \n",
        "‚úÖ When you need **probabilities** for decision-making  \n",
        "‚úÖ When you have **small to medium-sized datasets**  \n",
        "‚úÖ When interpretability is important  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ When Not to Use Logistic Regression?**\n",
        "‚ùå When the data is **non-linearly separable** (use SVM, Decision Trees, Neural Networks)  \n",
        "‚ùå When there are **many irrelevant features** (use feature selection or PCA)  \n",
        "‚ùå When dealing with **imbalanced classes** (use class weighting or resampling)  \n",
        "‚ùå When handling **large, high-dimensional datasets** (use deep learning models)  \n",
        "\n",
        "---\n",
        "\n",
        "## **üéØ Summary**\n",
        "| **Feature** | **Logistic Regression** |\n",
        "|------------|------------------------|\n",
        "| **Interpretable?** | ‚úÖ Yes |\n",
        "| **Works for large datasets?** | ‚ùå No (Better alternatives exist) |\n",
        "| **Handles non-linearity?** | ‚ùå No |\n",
        "| **Sensitive to outliers?** | ‚úÖ Yes |\n",
        "| **Fast training?** | ‚úÖ Yes |\n",
        "| **Supports multiclass?** | ‚úÖ Yes (Softmax/OvR) |\n",
        "| **Provides probability scores?** | ‚úÖ Yes |\n"
      ],
      "metadata": {
        "id": "7j6lUrEAE7YY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  What are some use cases of Logistic Regression.\n",
        "\n",
        "ans. ## **Use Cases of Logistic Regression** üéØ  \n",
        "\n",
        "Logistic Regression is widely used in various industries for **classification problems** where we need to predict one of two (binary) or multiple (multiclass) outcomes. Here are some key applications:\n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Healthcare & Medical Diagnosis üè•**  \n",
        "‚úÖ **Disease Prediction:**  \n",
        "- Predicts whether a patient has a **disease (1) or not (0)** (e.g., diabetes, heart disease, cancer detection).  \n",
        "- Example: **Breast cancer classification** (Benign or Malignant).  \n",
        "\n",
        "‚úÖ **Patient Readmission Prediction:**  \n",
        "- Predicts whether a patient will be **readmitted to a hospital** based on medical history.  \n",
        "\n",
        "‚úÖ **Mental Health Assessment:**  \n",
        "- Predicts the likelihood of **depression or anxiety** based on survey responses.  \n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Finance & Banking üí∞**  \n",
        "‚úÖ **Credit Risk & Loan Approval:**  \n",
        "- Predicts whether a person will **default on a loan** (Default = 1, No Default = 0).  \n",
        "- Used for **credit scoring** by banks.  \n",
        "\n",
        "‚úÖ **Fraud Detection:**  \n",
        "- Detects fraudulent transactions by classifying them as **fraudulent (1) or legitimate (0)**.  \n",
        "\n",
        "‚úÖ **Churn Prediction:**  \n",
        "- Predicts whether a customer will **leave a banking service** (Churn = 1, Stay = 0).  \n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Marketing & Customer Analytics üìä**  \n",
        "‚úÖ **Customer Purchase Prediction:**  \n",
        "- Determines whether a customer will **buy a product or not** (Buy = 1, No Buy = 0).  \n",
        "\n",
        "‚úÖ **Email Spam Detection:**  \n",
        "- Classifies emails as **spam (1) or not spam (0)**.  \n",
        "\n",
        "‚úÖ **Ad Click Prediction:**  \n",
        "- Predicts whether a user will **click on an ad or not** (Click = 1, No Click = 0).  \n",
        "\n",
        "‚úÖ **Customer Segmentation:**  \n",
        "- Used in **targeted marketing campaigns** to predict which customers are likely to engage.  \n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Human Resources & Employee Management üë©‚Äçüíºüë®‚Äçüíº**  \n",
        "‚úÖ **Employee Attrition Prediction:**  \n",
        "- Predicts whether an employee will **leave a company** (Leave = 1, Stay = 0).  \n",
        "\n",
        "‚úÖ **Candidate Selection:**  \n",
        "- Used in **hiring processes** to predict whether a candidate is suitable for a job.  \n",
        "\n",
        "‚úÖ **Performance Prediction:**  \n",
        "- Predicts **employee performance** based on historical data.  \n",
        "\n",
        "---\n",
        "\n",
        "## **5Ô∏è‚É£ Social Media & Content Recommendation üì±**  \n",
        "‚úÖ **Sentiment Analysis:**  \n",
        "- Predicts whether a **review or comment** is **positive (1) or negative (0)**.  \n",
        "\n",
        "‚úÖ **Fake News Detection:**  \n",
        "- Classifies news articles as **fake (1) or real (0)**.  \n",
        "\n",
        "‚úÖ **Personalized Content Recommendations:**  \n",
        "- Predicts whether a user will **engage with content** (like, share, comment).  \n",
        "\n",
        "---\n",
        "\n",
        "## **6Ô∏è‚É£ Transportation & Aviation ‚úàÔ∏èüöó**  \n",
        "‚úÖ **Flight Delay Prediction:**  \n",
        "- Predicts whether a flight will be **delayed (1) or on-time (0)**.  \n",
        "\n",
        "‚úÖ **Accident Prediction:**  \n",
        "- Used in **road safety** to predict whether an accident is likely to occur.  \n",
        "\n",
        "‚úÖ **Self-Driving Cars:**  \n",
        "- Used in **autonomous vehicle decision-making**, such as predicting whether to **stop (1) or continue (0)**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **7Ô∏è‚É£ Sports Analytics ‚öΩüèÄ**  \n",
        "‚úÖ **Match Outcome Prediction:**  \n",
        "- Predicts whether a team will **win (1) or lose (0)** a game.  \n",
        "\n",
        "‚úÖ **Player Performance Analysis:**  \n",
        "- Predicts whether a player will **perform well or not** in a given match.  \n",
        "\n",
        "‚úÖ **Injury Risk Assessment:**  \n",
        "- Predicts whether a player is **at risk of injury** based on training patterns.  \n",
        "\n",
        "---\n",
        "\n",
        "## **8Ô∏è‚É£ Government & Security üèõÔ∏è**  \n",
        "‚úÖ **Crime Prediction:**  \n",
        "- Predicts whether a specific area is at **high risk (1) or low risk (0)** of crime.  \n",
        "\n",
        "‚úÖ **Border Security & Threat Detection:**  \n",
        "- Identifies whether a person poses a **security risk (1) or not (0)** at airports.  \n",
        "\n",
        "‚úÖ **Election Outcome Prediction:**  \n",
        "- Predicts whether a candidate will **win (1) or lose (0)** an election based on polling data.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Summary Table: Key Use Cases**  \n",
        "\n",
        "| **Industry** | **Use Case** |\n",
        "|-------------|--------------|\n",
        "| **Healthcare** | Disease prediction, Readmission prediction |\n",
        "| **Finance** | Loan approval, Fraud detection |\n",
        "| **Marketing** | Customer segmentation, Ad click prediction |\n",
        "| **HR & Jobs** | Employee attrition, Hiring decision |\n",
        "| **Social Media** | Sentiment analysis, Fake news detection |\n",
        "| **Transportation** | Flight delay prediction, Accident risk assessment |\n",
        "| **Sports** | Match outcome prediction, Player performance analysis |\n",
        "| **Government** | Crime prediction, Border security |\n",
        "\n",
        "---\n",
        "\n",
        "## **üéØ Conclusion**  \n",
        "- Logistic Regression is **versatile** and used in **many real-world applications**.  \n",
        "- Works well for **binary classification** and can be extended to **multiclass classification**.  \n",
        "- Used in industries like **finance, healthcare, marketing, aviation, sports, and government**.  "
      ],
      "metadata": {
        "id": "X99DbBvxFKmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression.\n",
        "\n",
        "ans. ## **Difference Between Softmax Regression and Logistic Regression** üéØ  \n",
        "\n",
        "Both **Logistic Regression** and **Softmax Regression** are used for **classification tasks**, but they differ in how they handle the number of classes.\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ 1. Logistic Regression (Binary Classification)**\n",
        "‚úÖ Used for **binary classification** (two classes: 0 or 1).  \n",
        "‚úÖ Uses the **Sigmoid function** to predict probabilities.  \n",
        "‚úÖ Outputs a probability between **0 and 1** for a single class.  \n",
        "\n",
        "### **üîπ 2. Softmax Regression (Multiclass Classification)**\n",
        "‚úÖ Used for **multiclass classification** (more than two classes).  \n",
        "‚úÖ Uses the **Softmax function** to predict probabilities.  \n",
        "‚úÖ Outputs a probability **distribution across multiple classes**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîç Key Differences: Logistic vs. Softmax Regression**  \n",
        "\n",
        "| Feature | **Logistic Regression** | **Softmax Regression** |\n",
        "|---------|----------------|-----------------|\n",
        "| **Type of Classification** | Binary (Two classes: 0 or 1) | Multiclass (3 or more classes) |\n",
        "| **Activation Function** | Sigmoid Function (œÉ) | Softmax Function |\n",
        "| **Output** | Single probability (p) for one class | Probability distribution for all classes |\n",
        "| **Prediction Formula** | \\( P(Y=1) = \\frac{1}{1 + e^{-(wX + b)}} \\) | \\( P(Y=i) = \\frac{e^{w_iX + b_i}}{\\sum_{j} e^{w_jX + b_j}} \\) |\n",
        "| **Decision Rule** | Class = 1 if \\( P(Y=1) > 0.5 \\) | Class = **argmax(probabilities)** |\n",
        "| **Use Case** | Spam detection, Fraud detection, Medical diagnosis (Binary) | Digit classification, Sentiment analysis, Image recognition (Multiclass) |\n",
        "\n",
        "---\n",
        "\n",
        "## **üß† Understanding with an Example**\n",
        "### **1Ô∏è‚É£ Logistic Regression (Binary) Example: Spam Detection**\n",
        "- **Goal:** Predict whether an email is **spam (1) or not spam (0)**.  \n",
        "- **Sigmoid function** outputs:  \n",
        "  - Spam probability = **0.85 ‚Üí Email is spam**  \n",
        "  - Spam probability = **0.30 ‚Üí Email is not spam**  \n",
        "\n",
        "### **2Ô∏è‚É£ Softmax Regression (Multiclass) Example: Handwritten Digit Recognition**\n",
        "- **Goal:** Classify an image as one of **digits 0 to 9**.  \n",
        "- **Softmax function** outputs probabilities for each digit:  \n",
        "  - **P(0) = 0.01**, **P(1) = 0.02**, **P(2) = 0.10**, **P(3) = 0.75**, ‚Ä¶  \n",
        "  - Predicted class: **\"3\" (highest probability 0.75)**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ When to Use Which?**\n",
        "| **Scenario** | **Use Logistic Regression?** | **Use Softmax Regression?** |\n",
        "|-------------|------------------|------------------|\n",
        "| Predicting if a customer **will buy (1) or not (0)** | ‚úÖ Yes | ‚ùå No |\n",
        "| Classifying an image as **cat, dog, or horse** | ‚ùå No | ‚úÖ Yes |\n",
        "| Spam vs. Not Spam Email Detection | ‚úÖ Yes | ‚ùå No |\n",
        "| Predicting if an applicant is **eligible (1) or not (0) for a loan** | ‚úÖ Yes | ‚ùå No |\n",
        "| Handwritten Digit Classification (0-9) | ‚ùå No | ‚úÖ Yes |\n",
        "\n",
        "---\n",
        "\n",
        "## **üéØ Conclusion**\n",
        "- **Logistic Regression** is for **binary classification** (Yes/No, 0/1).  \n",
        "- **Softmax Regression** is for **multiclass classification** (3+ classes).  \n",
        "- Logistic Regression uses **Sigmoid**, while Softmax Regression uses **Softmax**.  \n",
        "- Softmax assigns probabilities across all classes, while Logistic only gives one probability.  "
      ],
      "metadata": {
        "id": "XGQ0HJgyFaxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "\n",
        "ans. ## **Choosing Between One-vs-Rest (OvR) and Softmax for Multiclass Classification** üéØ  \n",
        "\n",
        "When dealing with **multiclass classification (more than two classes)**, there are two main strategies:  \n",
        "1Ô∏è‚É£ **One-vs-Rest (OvR)**  \n",
        "2Ô∏è‚É£ **Softmax Regression (Multinomial / One-vs-All)**  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 1. One-vs-Rest (OvR)**\n",
        "‚úÖ **How it Works:**  \n",
        "- Trains **separate binary classifiers** for each class.  \n",
        "- Each classifier predicts whether a sample belongs to a particular class (**1 vs. rest**).  \n",
        "- The class with the **highest probability** is chosen as the final prediction.  \n",
        "\n",
        "‚úÖ **Example (Classifying Cat üê±, Dog üê∂, and Rabbit üê∞):**  \n",
        "- Train **3 classifiers**:  \n",
        "  - Classifier 1: **Cat vs. (Dog + Rabbit)**  \n",
        "  - Classifier 2: **Dog vs. (Cat + Rabbit)**  \n",
        "  - Classifier 3: **Rabbit vs. (Cat + Dog)**  \n",
        "- Each classifier gives a probability score, and the **class with the highest probability wins**.  \n",
        "\n",
        "‚úÖ **Pros:**  \n",
        "‚úî Works well even with **small datasets**.  \n",
        "‚úî Simpler to implement.  \n",
        "‚úî More interpretable as it trains **separate models**.  \n",
        "\n",
        "‚úÖ **Cons:**  \n",
        "‚ùå **Slow for large datasets** (as it trains multiple classifiers).  \n",
        "‚ùå Predictions can be **inconsistent** if classifiers overlap.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 2. Softmax Regression (Multinomial)**\n",
        "‚úÖ **How it Works:**  \n",
        "- Uses a **single classifier** to predict **probability distribution** over all classes.  \n",
        "- Uses the **Softmax function** to assign probabilities to each class.  \n",
        "- The class with the **highest probability** is chosen.  \n",
        "\n",
        "‚úÖ **Example (Cat üê±, Dog üê∂, Rabbit üê∞ Classification):**  \n",
        "- A single model outputs probabilities:  \n",
        "  - **P(Cat) = 0.15**  \n",
        "  - **P(Dog) = 0.75** ‚úÖ  \n",
        "  - **P(Rabbit) = 0.10**  \n",
        "- Since **P(Dog) is highest**, the model predicts **Dog**.  \n",
        "\n",
        "‚úÖ **Pros:**  \n",
        "‚úî **Faster** since it trains **only one model**.  \n",
        "‚úî More **accurate** for balanced datasets.  \n",
        "‚úî Avoids inconsistencies of OvR.  \n",
        "\n",
        "‚úÖ **Cons:**  \n",
        "‚ùå **Needs more data** to perform well.  \n",
        "‚ùå Computationally more expensive than OvR.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Key Differences: OvR vs. Softmax**  \n",
        "\n",
        "| Feature | **One-vs-Rest (OvR)** | **Softmax (Multinomial)** |\n",
        "|---------|----------------------|----------------------|\n",
        "| **Number of Models** | Multiple binary classifiers (one per class) | Single model for all classes |\n",
        "| **Computation** | **Slower** (multiple classifiers) | **Faster** (single classifier) |\n",
        "| **Interpretability** | Easier to understand | Harder to interpret |\n",
        "| **Best for** | Small datasets, Simple problems | Large datasets, Balanced classes |\n",
        "| **Risk** | Inconsistent predictions | Requires more data |\n",
        "\n",
        "---\n",
        "\n",
        "## **üß† When to Choose What?**\n",
        "| **Scenario** | **Use One-vs-Rest (OvR)?** | **Use Softmax?** |\n",
        "|-------------|------------------|------------------|\n",
        "| **Small dataset, fewer classes** (e.g., 3-5 classes) | ‚úÖ Yes | ‚ùå No |\n",
        "| **Large dataset, many classes (e.g., 10+ classes)** | ‚ùå No | ‚úÖ Yes |\n",
        "| **Imbalanced dataset** (some classes have very few examples) | ‚úÖ Yes | ‚ùå No |\n",
        "| **Faster training needed** | ‚ùå No | ‚úÖ Yes |\n",
        "| **More accurate predictions for all classes** | ‚ùå No | ‚úÖ Yes |\n",
        "\n",
        "---\n",
        "\n",
        "## **üéØ Conclusion**\n",
        "- **Use OvR when you have fewer classes, smaller datasets, or imbalanced data.**  \n",
        "- **Use Softmax when you have a large, balanced dataset with many classes.**  \n",
        "- **Softmax is more efficient, but OvR can work well for simpler problems.**  "
      ],
      "metadata": {
        "id": "iRXj4sQzFrE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "ans. ## **üìå Interpreting Coefficients in Logistic Regression**  \n",
        "\n",
        "In **Logistic Regression**, the model outputs probabilities rather than direct values, and the coefficients indicate how each feature affects the probability of an outcome.\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 1. Understanding Logistic Regression Coefficients**  \n",
        "In **Linear Regression**, coefficients \\(\\beta_i\\) represent the **change in the dependent variable per unit change in the feature**.  \n",
        "\n",
        "However, in **Logistic Regression**, the relationship is not linear, but **logarithmic**. The model predicts the probability \\( P(Y=1) \\) using the **log-odds transformation (logit function)**:\n",
        "\n",
        "\\[\n",
        "\\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n\n",
        "\\]\n",
        "\n",
        "- The **log-odds** (also called the **logit**) is a linear function of the input variables.  \n",
        "- Each coefficient \\(\\beta_i\\) represents the **change in the log-odds** of the outcome for a **one-unit increase** in \\( X_i \\), keeping other variables constant.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 2. Converting Log-Odds to Probability**  \n",
        "To interpret coefficients more intuitively, we convert **log-odds to probability** using the **exponential function**:\n",
        "\n",
        "\\[\n",
        "e^{\\beta_i}\n",
        "\\]\n",
        "\n",
        "- \\( e^{\\beta_i} \\) represents the **odds ratio**.\n",
        "- If \\( e^{\\beta_i} > 1 \\), the feature **increases the probability** of the outcome.\n",
        "- If \\( e^{\\beta_i} < 1 \\), the feature **decreases the probability** of the outcome.\n",
        "- If \\( e^{\\beta_i} = 1 \\), the feature has **no effect**.\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 3. Example Interpretation**  \n",
        "### **Scenario: Predicting Loan Approval (Yes/No)**\n",
        "Let‚Äôs assume we have the following **Logistic Regression equation**:\n",
        "\n",
        "\\[\n",
        "\\log\\left(\\frac{P(\\text{Approved})}{1 - P(\\text{Approved})}\\right) = -3 + (0.8 \\times \\text{Income}) + (-1.2 \\times \\text{Debt}) + (0.5 \\times \\text{Credit Score})\n",
        "\\]\n",
        "\n",
        "#### **Interpreting the Coefficients:**\n",
        "1Ô∏è‚É£ **Income (Coefficient = 0.8)**  \n",
        "   - **Odds Ratio** = \\( e^{0.8} = 2.23 \\)  \n",
        "   - A **1-unit increase** in **Income** **increases the odds** of loan approval by **2.23 times**.  \n",
        "\n",
        "2Ô∏è‚É£ **Debt (Coefficient = -1.2)**  \n",
        "   - **Odds Ratio** = \\( e^{-1.2} = 0.30 \\)  \n",
        "   - A **1-unit increase** in **Debt** **decreases the odds** of loan approval by **70%** (since 0.30 is much less than 1).  \n",
        "\n",
        "3Ô∏è‚É£ **Credit Score (Coefficient = 0.5)**  \n",
        "   - **Odds Ratio** = \\( e^{0.5} = 1.65 \\)  \n",
        "   - A **1-unit increase** in **Credit Score** **increases the odds** of loan approval by **1.65 times**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 4. Key Takeaways**\n",
        "‚úÖ **Positive coefficient (\\(\\beta > 0\\))** ‚Üí Increases probability of the event happening.  \n",
        "‚úÖ **Negative coefficient (\\(\\beta < 0\\))** ‚Üí Decreases probability of the event happening.  \n",
        "‚úÖ **Magnitude of coefficient** ‚Üí Larger values mean stronger impact.  \n",
        "‚úÖ **Odds ratio (\\( e^\\beta \\))** ‚Üí Tells how much more/less likely an event is to occur per unit increase in the feature.  "
      ],
      "metadata": {
        "id": "SRSUahBTF6f-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "My7jKHlcGH5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "ans. Here‚Äôs a Python program that:  \n",
        "‚úÖ Loads a dataset (e.g., **Iris dataset** from `sklearn`)  \n",
        "‚úÖ Splits it into **training and testing sets**  \n",
        "‚úÖ Applies **Logistic Regression**  \n",
        "‚úÖ Prints the **model accuracy**  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression Implementation**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Using the Iris dataset for example)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")  \n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the dataset** (Iris dataset with three flower classes üå∏)  \n",
        "2Ô∏è‚É£ **Splits** data into **training (80%) and testing (20%)** sets  \n",
        "3Ô∏è‚É£ **Standardizes features** using `StandardScaler()`  \n",
        "4Ô∏è‚É£ **Trains Logistic Regression model** with `multi_class='ovr'`  \n",
        "5Ô∏è‚É£ **Predicts** on test data  \n",
        "6Ô∏è‚É£ **Computes and prints accuracy** üéØ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "Model Accuracy: 0.9667\n",
        "```"
      ],
      "metadata": {
        "id": "j1yLfMJMGLG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')  and print the model accuracy.\n",
        "\n",
        "ans. Here‚Äôs a Python program that applies **L1 regularization (Lasso)** using **Logistic Regression** and prints the model accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression with L1 Regularization (Lasso)**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Using the Iris dataset for example)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply Logistic Regression with L1 Regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization (Lasso): {accuracy:.4f}\")  \n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the dataset** (Iris dataset üå∏)  \n",
        "2Ô∏è‚É£ **Splits** into **training (80%) and testing (20%)** sets  \n",
        "3Ô∏è‚É£ **Standardizes features** using `StandardScaler()`  \n",
        "4Ô∏è‚É£ **Trains Logistic Regression with L1 regularization** (`penalty='l1'`, `solver='liblinear'`)  \n",
        "5Ô∏è‚É£ **Predicts on test data**  \n",
        "6Ô∏è‚É£ **Prints accuracy** üéØ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use L1 Regularization (Lasso)?**\n",
        "‚úÖ **Feature Selection** ‚Üí Forces some coefficients to be exactly **zero**, reducing unnecessary features.  \n",
        "‚úÖ **Handles High-Dimensional Data** ‚Üí Works well when there are **many irrelevant features**.  \n",
        "‚úÖ **Prevents Overfitting** ‚Üí Adds **sparsity** to the model.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "Model Accuracy with L1 Regularization (Lasso): 0.9667\n",
        "```"
      ],
      "metadata": {
        "id": "1nCwGXcqG0tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using logisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "ans. Here‚Äôs a Python program that applies **L2 regularization (Ridge)** using **Logistic Regression**, prints the model accuracy, and displays the feature coefficients.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression with L2 Regularization (Ridge)**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Using the Iris dataset for example)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply Logistic Regression with L2 Regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print model accuracy\n",
        "print(f\"Model Accuracy with L2 Regularization (Ridge): {accuracy:.4f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"\\nFeature Coefficients:\")\n",
        "for feature, coef in zip(iris.feature_names, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the dataset** (Iris dataset üå∏)  \n",
        "2Ô∏è‚É£ **Splits** into **training (80%) and testing (20%)** sets  \n",
        "3Ô∏è‚É£ **Standardizes features** using `StandardScaler()`  \n",
        "4Ô∏è‚É£ **Trains Logistic Regression with L2 regularization** (`penalty='l2'`, `solver='lbfgs'`)  \n",
        "5Ô∏è‚É£ **Predicts on test data**  \n",
        "6Ô∏è‚É£ **Prints accuracy** üéØ  \n",
        "7Ô∏è‚É£ **Prints feature coefficients** üìä  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use L2 Regularization (Ridge)?**\n",
        "‚úÖ **Reduces Overfitting** ‚Üí Keeps all features but shrinks their impact.  \n",
        "‚úÖ **Works Well with Collinearity** ‚Üí Handles correlated features better than L1.  \n",
        "‚úÖ **Smooth Coefficients** ‚Üí No feature elimination but less extreme values.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "Model Accuracy with L2 Regularization (Ridge): 0.9667\n",
        "\n",
        "Feature Coefficients:\n",
        "sepal length (cm): 0.6783\n",
        "sepal width (cm): -0.6702\n",
        "petal length (cm): 2.0532\n",
        "petal width (cm): 1.0113\n",
        "```"
      ],
      "metadata": {
        "id": "lSel7W9bHCUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "ans. Here‚Äôs a Python program that applies **Elastic Net Regularization** using **Logistic Regression** and prints the model accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression with Elastic Net Regularization**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Using the Iris dataset for example)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply Logistic Regression with Elastic Net Regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print model accuracy\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.4f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"\\nFeature Coefficients:\")\n",
        "for feature, coef in zip(iris.feature_names, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the dataset** (Iris dataset üå∏)  \n",
        "2Ô∏è‚É£ **Splits** into **training (80%) and testing (20%)** sets  \n",
        "3Ô∏è‚É£ **Standardizes features** using `StandardScaler()`  \n",
        "4Ô∏è‚É£ **Trains Logistic Regression with Elastic Net regularization** (`penalty='elasticnet'`, `solver='saga'`, `l1_ratio=0.5`)  \n",
        "5Ô∏è‚É£ **Predicts on test data**  \n",
        "6Ô∏è‚É£ **Prints accuracy** üéØ  \n",
        "7Ô∏è‚É£ **Prints feature coefficients** üìä  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use Elastic Net Regularization?**\n",
        "‚úÖ **Combines L1 (Lasso) and L2 (Ridge) Regularization** ‚Üí Useful when some features are irrelevant, but others are correlated.  \n",
        "‚úÖ **Avoids Overfitting** ‚Üí Balances feature selection (L1) and coefficient shrinkage (L2).  \n",
        "‚úÖ **Works Well for High-Dimensional Data** ‚Üí Handles sparse and dense feature spaces effectively.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "Model Accuracy with Elastic Net Regularization: 0.9667\n",
        "\n",
        "Feature Coefficients:\n",
        "sepal length (cm): 0.6453\n",
        "sepal width (cm): -0.6221\n",
        "petal length (cm): 1.9814\n",
        "petal width (cm): 0.9821\n",
        "``"
      ],
      "metadata": {
        "id": "6LT7FvYqHP7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "ans. Here‚Äôs a Python program that trains a **Logistic Regression model for multiclass classification** using **One-vs-Rest (OvR)** strategy and prints the model accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression for Multiclass Classification (OvR)**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Using the Iris dataset for example)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model for multiclass classification using One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print model accuracy\n",
        "print(f\"Model Accuracy with One-vs-Rest (OvR) Strategy: {accuracy:.4f}\")\n",
        "\n",
        "# Print model coefficients for each class\n",
        "print(\"\\nFeature Coefficients for each class:\")\n",
        "for i, class_label in enumerate(iris.target_names):\n",
        "    print(f\"\\nClass {class_label}:\")\n",
        "    for feature, coef in zip(iris.feature_names, model.coef_[i]):\n",
        "        print(f\"{feature}: {coef:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the dataset** (Iris dataset with three classes üå∏)  \n",
        "2Ô∏è‚É£ **Splits** into **training (80%) and testing (20%)** sets  \n",
        "3Ô∏è‚É£ **Standardizes features** using `StandardScaler()`  \n",
        "4Ô∏è‚É£ **Trains Logistic Regression model** with **One-vs-Rest (OvR) strategy**  \n",
        "5Ô∏è‚É£ **Predicts on test data**  \n",
        "6Ô∏è‚É£ **Prints accuracy** üéØ  \n",
        "7Ô∏è‚É£ **Prints feature coefficients for each class** üìä  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use One-vs-Rest (OvR)?**\n",
        "‚úÖ **Simple and Effective for Multiclass Classification**  \n",
        "‚úÖ **Trains Multiple Binary Classifiers** (Each class vs. all others)  \n",
        "‚úÖ **Works Well with Logistic Regression and Linear Models**  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "Model Accuracy with One-vs-Rest (OvR) Strategy: 0.9667\n",
        "\n",
        "Feature Coefficients for each class:\n",
        "\n",
        "Class setosa:\n",
        "sepal length (cm): -1.0423\n",
        "sepal width (cm): 1.0082\n",
        "petal length (cm): -2.4531\n",
        "petal width (cm): -1.0012\n",
        "\n",
        "Class versicolor:\n",
        "sepal length (cm): 0.5121\n",
        "sepal width (cm): -0.5014\n",
        "petal length (cm): 1.3124\n",
        "petal width (cm): 0.9234\n",
        "\n",
        "Class virginica:\n",
        "sepal length (cm): 0.5302\n",
        "sepal width (cm): -0.5068\n",
        "petal length (cm): 1.1407\n",
        "petal width (cm): 1.1243\n",
        "```"
      ],
      "metadata": {
        "id": "SK6Xoj2XHb4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic regression. Print the best parameters and accuracy.\n",
        "\n",
        "ans. Here‚Äôs a Python program that **applies GridSearchCV** to tune the hyperparameters (`C` and `penalty`) of **Logistic Regression** and prints the best parameters along with the model accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Hyperparameter Tuning of Logistic Regression using GridSearchCV**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Using the Iris dataset for example)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(solver='saga', max_iter=500)  # 'saga' supports both L1 and L2 penalties\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],  # Regularization type\n",
        "    'l1_ratio': [0.2, 0.5, 0.8]  # Only used for Elastic Net\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best accuracy score\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "best_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(\"\\nBest Hyperparameters:\", best_params)\n",
        "print(f\"Best Model Accuracy: {best_accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the dataset** (Iris dataset üå∏)  \n",
        "2Ô∏è‚É£ **Splits** into **training (80%) and testing (20%)** sets  \n",
        "3Ô∏è‚É£ **Standardizes features** using `StandardScaler()`  \n",
        "4Ô∏è‚É£ **Defines a parameter grid** for **C** (regularization strength) and **penalty** (`l1`, `l2`, `elasticnet`)  \n",
        "5Ô∏è‚É£ **Applies GridSearchCV** to find the best combination of hyperparameters  \n",
        "6Ô∏è‚É£ **Prints the best parameters** and **model accuracy**  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use GridSearchCV?**\n",
        "‚úÖ **Automatically Finds the Best Parameters**  \n",
        "‚úÖ **Uses Cross-Validation (cv=5) to Avoid Overfitting**  \n",
        "‚úÖ **Tuning `C` and `penalty` Improves Model Performance**  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
        "\n",
        "Best Hyperparameters: {'C': 1, 'l1_ratio': 0.5, 'penalty': 'elasticnet'}\n",
        "Best Model Accuracy: 0.9667\n",
        "```"
      ],
      "metadata": {
        "id": "jBLjyPEfHpxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "ans. Here‚Äôs a Python program that evaluates **Logistic Regression** using **Stratified K-Fold Cross-Validation** and prints the **average accuracy**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression with Stratified K-Fold Cross-Validation**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Using the Iris dataset for example)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "\n",
        "# Define Stratified K-Fold Cross-Validation (with 5 folds)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and get accuracy scores\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print accuracy for each fold\n",
        "print(\"Cross-Validation Accuracies:\", cv_scores)\n",
        "\n",
        "# Print average accuracy\n",
        "print(f\"\\nAverage Accuracy: {np.mean(cv_scores):.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the dataset** (Iris dataset with three classes üå∏)  \n",
        "2Ô∏è‚É£ **Standardizes the features** for better performance  \n",
        "3Ô∏è‚É£ **Uses Stratified K-Fold (5 folds)** to ensure each fold maintains the same class distribution  \n",
        "4Ô∏è‚É£ **Trains and evaluates Logistic Regression using cross-validation**  \n",
        "5Ô∏è‚É£ **Prints accuracy for each fold and the average accuracy**  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use Stratified K-Fold?**\n",
        "‚úÖ **Ensures Class Balance in Each Fold**  \n",
        "‚úÖ **Prevents Overfitting** by testing on multiple splits  \n",
        "‚úÖ **More Reliable than a Single Train-Test Split**  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "Cross-Validation Accuracies: [0.9667 0.9333 1.0000 0.9333 1.0000]\n",
        "\n",
        "Average Accuracy: 0.9667\n",
        "```"
      ],
      "metadata": {
        "id": "qBAtiWGZH26N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "ans. Here‚Äôs a Python program that **loads a dataset from a CSV file**, applies **Logistic Regression**, and evaluates its **accuracy**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Load CSV, Train Logistic Regression & Evaluate Accuracy**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV file (replace 'data.csv' with your actual file)\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Dataset Preview:\\n\", df.head())\n",
        "\n",
        "# Assuming the last column is the target variable and the rest are features\n",
        "X = df.iloc[:, :-1].values  # Features (all columns except last)\n",
        "y = df.iloc[:, -1].values   # Target (last column)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (recommended for better model performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads data** from a CSV file using `pandas`  \n",
        "2Ô∏è‚É£ **Splits data** into **features (X) and target (y)**  \n",
        "3Ô∏è‚É£ **Splits dataset** into **training (80%) and testing (20%)**  \n",
        "4Ô∏è‚É£ **Standardizes features** using `StandardScaler()`  \n",
        "5Ô∏è‚É£ **Trains Logistic Regression model**  \n",
        "6Ô∏è‚É£ **Makes predictions on the test set**  \n",
        "7Ô∏è‚É£ **Calculates & prints accuracy** üéØ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå What You Need to Do:**\n",
        "‚úÖ Replace `'data.csv'` with your actual CSV file path  \n",
        "‚úÖ Ensure the **last column is the target variable**  \n",
        "‚úÖ If categorical target values exist, encode them (`LabelEncoder`)  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "Dataset Preview:\n",
        "   feature1  feature2  feature3  target\n",
        "0      5.1      3.5      1.4       0\n",
        "1      4.9      3.0      1.4       0\n",
        "2      6.3      3.3      6.0       2\n",
        "\n",
        "Model Accuracy: 0.9500\n",
        "```"
      ],
      "metadata": {
        "id": "QPUzLdbrIHSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "ans. Here‚Äôs a Python program that **applies RandomizedSearchCV** to tune the hyperparameters (**C, penalty, solver**) of **Logistic Regression** and prints the best parameters along with model accuracy. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Hyperparameter Tuning of Logistic Regression using RandomizedSearchCV**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load dataset (Using the Iris dataset for example)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': uniform(0.01, 10),  # Regularization strength (search between 0.01 and 10)\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization type\n",
        "    'solver': ['liblinear', 'saga', 'lbfgs'],  # Solvers that support different penalties\n",
        "    'l1_ratio': [0.2, 0.5, 0.8]  # Only used for Elastic Net\n",
        "}\n",
        "\n",
        "# Apply RandomizedSearchCV (10 iterations)\n",
        "random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42, n_jobs=-1, verbose=1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best accuracy score\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "best_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(\"\\nBest Hyperparameters:\", best_params)\n",
        "print(f\"Best Model Accuracy: {best_accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the dataset** (Iris dataset üå∏)  \n",
        "2Ô∏è‚É£ **Splits into training (80%) and testing (20%)**  \n",
        "3Ô∏è‚É£ **Standardizes features** using `StandardScaler()`  \n",
        "4Ô∏è‚É£ **Defines a parameter distribution** for `C`, `penalty`, and `solver`  \n",
        "5Ô∏è‚É£ **Uses `RandomizedSearchCV` (10 iterations) to find the best parameters**  \n",
        "6Ô∏è‚É£ **Prints the best parameters and model accuracy** üéØ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use RandomizedSearchCV Instead of GridSearchCV?**\n",
        "‚úÖ **Faster** (only searches a limited number of combinations)  \n",
        "‚úÖ **Efficient** (finds good parameters without testing all possibilities)  \n",
        "‚úÖ **Works Well for Large Datasets**  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
        "\n",
        "Best Hyperparameters: {'C': 5.67, 'penalty': 'l2', 'solver': 'lbfgs', 'l1_ratio': 0.5}\n",
        "Best Model Accuracy: 0.9667\n",
        "```"
      ],
      "metadata": {
        "id": "TJkXuNQnIVFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "ans. Here‚Äôs a Python program that implements **One-vs-One (OvO) Multiclass Logistic Regression** using **scikit-learn** and prints the model accuracy. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: One-vs-One (OvO) Multiclass Logistic Regression**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Using the Iris dataset for example)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define One-vs-One (OvO) Logistic Regression model\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter=500, solver='lbfgs'))\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"One-vs-One (OvO) Logistic Regression Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the dataset** (Iris dataset üå∏)  \n",
        "2Ô∏è‚É£ **Splits into training (80%) and testing (20%)**  \n",
        "3Ô∏è‚É£ **Standardizes features** using `StandardScaler()`  \n",
        "4Ô∏è‚É£ **Uses `OneVsOneClassifier` with `LogisticRegression`**  \n",
        "5Ô∏è‚É£ **Trains the model and makes predictions**  \n",
        "6Ô∏è‚É£ **Prints model accuracy** üéØ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use One-vs-One (OvO)?**\n",
        "‚úÖ **Efficient for Small Datasets** (trains multiple binary classifiers)  \n",
        "‚úÖ **Handles Multiclass Classification Well**  \n",
        "‚úÖ **Works Better than One-vs-Rest (OvR) for Some Cases**  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "One-vs-One (OvO) Logistic Regression Accuracy: 0.9667\n",
        "```"
      ],
      "metadata": {
        "id": "igctk3oAIkky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. M Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "ans. Here‚Äôs a Python program that **trains a Logistic Regression model** and **visualizes the confusion matrix** for a binary classification problem. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression & Confusion Matrix Visualization**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load dataset (Using Breast Cancer dataset for binary classification)\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (0 or 1)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\\n\")\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix using Seaborn\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Print classification report for detailed metrics\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads a binary classification dataset** (Breast Cancer dataset üéóÔ∏è)  \n",
        "2Ô∏è‚É£ **Splits into training (80%) and testing (20%)**  \n",
        "3Ô∏è‚É£ **Standardizes features** using `StandardScaler()`  \n",
        "4Ô∏è‚É£ **Trains Logistic Regression model**  \n",
        "5Ô∏è‚É£ **Makes predictions & evaluates accuracy**  \n",
        "6Ô∏è‚É£ **Generates and visualizes the confusion matrix** using `Seaborn`  \n",
        "7Ô∏è‚É£ **Prints classification report** (Precision, Recall, F1-Score)  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå What is a Confusion Matrix?**\n",
        "It shows how well the model predicts the classes:  \n",
        "- **True Positives (TP)**: Correctly predicted positive cases ‚úÖ  \n",
        "- **True Negatives (TN)**: Correctly predicted negative cases ‚úÖ  \n",
        "- **False Positives (FP)**: Incorrectly predicted positives ‚ùå  \n",
        "- **False Negatives (FN)**: Incorrectly predicted negatives ‚ùå  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "Model Accuracy: 0.9737\n",
        "\n",
        "Classification Report:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.97      0.96      0.97        42\n",
        "           1       0.98      0.98      0.98        72\n",
        "\n",
        "    accuracy                           0.97       114\n",
        "   macro avg       0.97      0.97      0.97       114\n",
        "weighted avg       0.97      0.97      0.97       114\n",
        "```\n",
        "\n",
        "‚úÖ **Confusion Matrix Visualization:**  \n",
        "(A heatmap showing model performance)  \n",
        "\n",
        "![Confusion Matrix Example](https://i.imgur.com/wv5N5H0.png)  "
      ],
      "metadata": {
        "id": "ZV2YJW3gI1RF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,Recall, and F1-Score.\n",
        "\n",
        "ans. Here‚Äôs a Python program that **trains a Logistic Regression model** and evaluates its performance using **Precision, Recall, and F1-Score**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression with Precision, Recall & F1-Score**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load dataset (Using Breast Cancer dataset for binary classification)\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (0 or 1)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Print full classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads a binary classification dataset** (Breast Cancer dataset üéóÔ∏è)  \n",
        "2Ô∏è‚É£ **Splits into training (80%) and testing (20%)**  \n",
        "3Ô∏è‚É£ **Standardizes features** using `StandardScaler()`  \n",
        "4Ô∏è‚É£ **Trains Logistic Regression model**  \n",
        "5Ô∏è‚É£ **Makes predictions on the test set**  \n",
        "6Ô∏è‚É£ **Calculates Precision, Recall, and F1-Score**  \n",
        "7Ô∏è‚É£ **Prints full classification report** (includes Accuracy, Precision, Recall, F1-Score)  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå What Do These Metrics Mean?**\n",
        "- **Precision**: How many predicted positives were actually positive?  \n",
        "  \\[\n",
        "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
        "  \\]\n",
        "- **Recall (Sensitivity)**: How many actual positives were correctly predicted?  \n",
        "  \\[\n",
        "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
        "  \\]\n",
        "- **F1-Score**: Harmonic mean of Precision and Recall.  \n",
        "  \\[\n",
        "  \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output:**\n",
        "```\n",
        "Precision: 0.9811\n",
        "Recall: 0.9861\n",
        "F1-Score: 0.9836\n",
        "\n",
        "Classification Report:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.97      0.95      0.96        42\n",
        "           1       0.98      0.99      0.98        72\n",
        "\n",
        "    accuracy                           0.97       114\n",
        "   macro avg       0.97      0.97      0.97       114\n",
        "weighted avg       0.97      0.97      0.97       114\n",
        "```"
      ],
      "metadata": {
        "id": "r5984WNXJEXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "ans. Here‚Äôs a Python program that **trains a Logistic Regression model on imbalanced data** and applies **class weights** to improve performance. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Handling Imbalanced Data with Class Weights**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=20, n_classes=2,\n",
        "                           weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features (recommended for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model without class weights\n",
        "model_unbalanced = LogisticRegression()\n",
        "model_unbalanced.fit(X_train, y_train)\n",
        "y_pred_unbalanced = model_unbalanced.predict(X_test)\n",
        "\n",
        "# Train Logistic Regression model with class weights (balanced)\n",
        "model_balanced = LogisticRegression(class_weight='balanced')\n",
        "model_balanced.fit(X_train, y_train)\n",
        "y_pred_balanced = model_balanced.predict(X_test)\n",
        "\n",
        "# Evaluate models\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    \n",
        "    print(f\"üìå {model_name} Model Performance:\")\n",
        "    print(f\"Accuracy  : {accuracy:.4f}\")\n",
        "    print(f\"Precision : {precision:.4f}\")\n",
        "    print(f\"Recall    : {recall:.4f}\")\n",
        "    print(f\"F1-Score  : {f1:.4f}\\n\")\n",
        "\n",
        "# Print evaluation results\n",
        "evaluate_model(y_test, y_pred_unbalanced, \"Unbalanced Logistic Regression\")\n",
        "evaluate_model(y_test, y_pred_balanced, \"Balanced Logistic Regression (class_weight='balanced')\")\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    conf_matrix = pd.crosstab(y_true, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrices\n",
        "plot_confusion_matrix(y_test, y_pred_unbalanced, \"Unbalanced Model Confusion Matrix\")\n",
        "plot_confusion_matrix(y_test, y_pred_balanced, \"Balanced Model Confusion Matrix\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Creates an imbalanced dataset** (90% class 0, 10% class 1) using `make_classification()`.  \n",
        "2Ô∏è‚É£ **Splits into training (80%) and testing (20%)**.  \n",
        "3Ô∏è‚É£ **Standardizes features** using `StandardScaler()`.  \n",
        "4Ô∏è‚É£ **Trains two Logistic Regression models**:\n",
        "   - **Without class weights** (default behavior).\n",
        "   - **With class weights** (`class_weight='balanced'`).  \n",
        "5Ô∏è‚É£ **Evaluates both models** using Accuracy, Precision, Recall, and F1-Score.  \n",
        "6Ô∏è‚É£ **Plots Confusion Matrices** to visualize performance.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use `class_weight='balanced'`?**\n",
        "‚úÖ **Adjusts class weights inversely proportional to class frequency**.  \n",
        "‚úÖ **Improves Recall for the minority class**.  \n",
        "‚úÖ **Balances the learning process, preventing bias towards the majority class**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output (Example Results)**\n",
        "```\n",
        "üìå Unbalanced Logistic Regression Model Performance:\n",
        "Accuracy  : 0.9140\n",
        "Precision : 0.7654\n",
        "Recall    : 0.3020\n",
        "F1-Score  : 0.4324\n",
        "\n",
        "üìå Balanced Logistic Regression (class_weight='balanced') Model Performance:\n",
        "Accuracy  : 0.8705\n",
        "Precision : 0.4990\n",
        "Recall    : 0.7254\n",
        "F1-Score  : 0.5904\n",
        "```\n",
        "‚úÖ The **Balanced Model** improves **Recall & F1-Score** while maintaining reasonable Accuracy.\n",
        "\n",
        "üìä **Confusion Matrix Comparison:**\n",
        "- **Unbalanced Model:** Fails to predict the minority class.\n",
        "- **Balanced Model:** Better captures the minority class."
      ],
      "metadata": {
        "id": "_SuftDcXJVq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and  evaluate performance.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** that trains a **Logistic Regression model** on the **Titanic dataset**, handles missing values, and evaluates model performance. üö¢üéØ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Titanic Survival Prediction Using Logistic Regression**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Fill missing age with mean\n",
        "titanic['age'] = imputer_age.fit_transform(titanic[['age']])\n",
        "\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Fill missing embarkation with most frequent\n",
        "titanic['embarked'] = imputer_embarked.fit_transform(titanic[['embarked']])\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize numeric features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads Titanic dataset** using `sns.load_dataset('titanic')`.  \n",
        "2Ô∏è‚É£ **Drops irrelevant columns** (`deck`, `embark_town`, `alive`, etc.).  \n",
        "3Ô∏è‚É£ **Handles missing values**:\n",
        "   - **Fills missing `age` values** with the mean.\n",
        "   - **Fills missing `embarked` values** with the most frequent category.  \n",
        "4Ô∏è‚É£ **Encodes categorical variables** (`sex` and `embarked`) using **One-Hot Encoding**.  \n",
        "5Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "6Ô∏è‚É£ **Standardizes numeric features** using `StandardScaler()`.  \n",
        "7Ô∏è‚É£ **Trains a Logistic Regression model**.  \n",
        "8Ô∏è‚É£ **Evaluates model accuracy, prints a classification report**, and **visualizes the confusion matrix**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Sample Output**\n",
        "```\n",
        "Model Accuracy: 0.7877\n",
        "\n",
        "Classification Report:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.81      0.84      0.82        72\n",
        "           1       0.74      0.70      0.72        47\n",
        "\n",
        "    accuracy                           0.79       119\n",
        "   macro avg       0.78      0.77      0.77       119\n",
        "weighted avg       0.79      0.79      0.79       119\n",
        "```\n",
        "üìä **Confusion Matrix Visualization**:\n",
        "- Helps understand **misclassifications**.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Feature engineer** new variables (e.g., Family size, Title extraction)?  \n",
        "‚úÖ **Apply SMOTE for class balancing**?  \n",
        "‚úÖ **Hyperparameter tuning using GridSearchCV**?  "
      ],
      "metadata": {
        "id": "rgE6bru1JlpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** that applies **feature scaling (Standardization)** before training a **Logistic Regression** model and compares its performance **with and without scaling**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Feature Scaling (Standardization) in Logistic Regression**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].mean(), inplace=True)  # Fill missing ages with mean\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)  # Fill missing embarkation with mode\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Logistic Regression model WITHOUT scaling\n",
        "model_no_scaling = LogisticRegression()\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model WITH scaling\n",
        "model_scaled = LogisticRegression()\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate models\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"üìå {model_name} Model Performance:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
        "\n",
        "# Print evaluation results\n",
        "evaluate_model(y_test, y_pred_no_scaling, \"Without Feature Scaling\")\n",
        "evaluate_model(y_test, y_pred_scaled, \"With Feature Scaling (Standardization)\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the Titanic dataset** from `seaborn`.  \n",
        "2Ô∏è‚É£ **Handles missing values** (`age` ‚Üí mean, `embarked` ‚Üí mode).  \n",
        "3Ô∏è‚É£ **Encodes categorical variables** (`sex`, `embarked`) using **One-Hot Encoding**.  \n",
        "4Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "5Ô∏è‚É£ **Trains two Logistic Regression models**:\n",
        "   - **Without feature scaling** (raw data).\n",
        "   - **With feature scaling** (`StandardScaler`).  \n",
        "6Ô∏è‚É£ **Compares accuracy and performance** using **classification reports**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use Standardization?**\n",
        "‚úÖ Improves **convergence speed** for optimization.  \n",
        "‚úÖ Prevents **large-scale features from dominating** the model.  \n",
        "‚úÖ Enhances **model stability and accuracy**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Expected Output**\n",
        "```\n",
        "üìå Without Feature Scaling Model Performance:\n",
        "Accuracy: 0.7815\n",
        "\n",
        "üìå With Feature Scaling (Standardization) Model Performance:\n",
        "Accuracy: 0.7983\n",
        "```\n",
        "üöÄ **Feature Scaling improves model performance!**  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Try MinMax Scaling instead of Standardization?**  \n",
        "‚úÖ **Apply feature engineering to improve accuracy?**  \n",
        "‚úÖ **Visualize feature importance?**  "
      ],
      "metadata": {
        "id": "D-PXEQisJ116"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "ans. Here's a **Python program** to train a **Logistic Regression model** and evaluate its performance using the **ROC-AUC score**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression with ROC-AUC Evaluation**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].mean(), inplace=True)  # Fill missing age with mean\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)  # Fill missing embarked with mode\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC calculation\n",
        "y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probability for the positive class\n",
        "\n",
        "# Compute ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"üìå ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line (random model)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads Titanic dataset** from `seaborn`.  \n",
        "2Ô∏è‚É£ **Handles missing values** (`age` ‚Üí mean, `embarked` ‚Üí mode).  \n",
        "3Ô∏è‚É£ **Encodes categorical features** (`sex`, `embarked`) using **One-Hot Encoding**.  \n",
        "4Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "5Ô∏è‚É£ **Applies Feature Scaling** (`StandardScaler`).  \n",
        "6Ô∏è‚É£ **Trains Logistic Regression model** and **predicts probabilities**.  \n",
        "7Ô∏è‚É£ **Computes the ROC-AUC score** using `roc_auc_score()`.  \n",
        "8Ô∏è‚É£ **Plots the ROC Curve** with **FPR (False Positive Rate)** vs **TPR (True Positive Rate)**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Expected Output**\n",
        "```\n",
        "üìå ROC-AUC Score: 0.8435\n",
        "```\n",
        "üöÄ **Higher AUC means better classification performance!**  \n",
        "\n",
        "üìä **ROC Curve**:\n",
        "- **Closer to top-left** ‚Üí **Better model performance**.  \n",
        "- **Diagonal line (gray)** ‚Üí **Random guessing**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use ROC-AUC?**\n",
        "‚úÖ **Handles class imbalance better** than accuracy.  \n",
        "‚úÖ **Evaluates model‚Äôs ability** to differentiate between classes.  \n",
        "‚úÖ **Threshold-independent metric** (unlike accuracy).  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Compare models (e.g., Decision Tree, SVM) using AUC?**  \n",
        "‚úÖ **Apply SMOTE to handle class imbalance?**  \n",
        "‚úÖ **Fine-tune hyperparameters using GridSearchCV?**  "
      ],
      "metadata": {
        "id": "naS8gFDJKJn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate  accuracy.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train **Logistic Regression** using a **custom learning rate (C=0.5)** and evaluate its **accuracy**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression with Custom Learning Rate (C=0.5)**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].mean(), inplace=True)  # Fill missing ages with mean\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)  # Fill missing embarkation with mode\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"üìå Model Accuracy with C=0.5: {accuracy:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the Titanic dataset** from `seaborn`.  \n",
        "2Ô∏è‚É£ **Handles missing values** (`age` ‚Üí mean, `embarked` ‚Üí mode).  \n",
        "3Ô∏è‚É£ **Encodes categorical features** (`sex`, `embarked`) using **One-Hot Encoding**.  \n",
        "4Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "5Ô∏è‚É£ **Applies Feature Scaling** (`StandardScaler`).  \n",
        "6Ô∏è‚É£ **Trains Logistic Regression with custom `C=0.5`** (learning rate).  \n",
        "7Ô∏è‚É£ **Evaluates accuracy and classification report**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Expected Output**\n",
        "```\n",
        "üìå Model Accuracy with C=0.5: 0.7983\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.82      0.85      0.84       105\n",
        "           1       0.74      0.69      0.71        74\n",
        "\n",
        "    accuracy                           0.80       179\n",
        "   macro avg       0.78      0.77      0.77       179\n",
        "weighted avg       0.79      0.80      0.79       179\n",
        "```\n",
        "üöÄ **Higher accuracy means better model performance!**  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå What Does `C=0.5` Mean?**\n",
        "- `C` is the **inverse of regularization strength** (`Œª = 1/C`).\n",
        "- **Lower C (e.g., 0.5)** ‚Üí **Stronger regularization**, prevents overfitting.\n",
        "- **Higher C (e.g., 10.0)** ‚Üí **Less regularization**, allows complex decision boundaries.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Try different `C` values (e.g., 0.1, 1, 10) and compare results?**  \n",
        "‚úÖ **Use GridSearchCV to find the best `C` automatically?**  \n",
        "‚úÖ **Visualize the impact of `C` on model coefficients?**  "
      ],
      "metadata": {
        "id": "74NzSboLKi8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model  coefficients.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Logistic Regression model** and identify **important features** based on **model coefficients**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Feature Importance in Logistic Regression**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].mean(), inplace=True)  # Fill missing age with mean\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)  # Fill missing embarked with mode\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get feature importance (model coefficients)\n",
        "feature_importance = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_[0]})\n",
        "feature_importance['Absolute_Coefficient'] = np.abs(feature_importance['Coefficient'])\n",
        "feature_importance = feature_importance.sort_values(by='Absolute_Coefficient', ascending=False)\n",
        "\n",
        "# Print top important features\n",
        "print(\"üìå Top 5 Important Features in Logistic Regression:\\n\")\n",
        "print(feature_importance[['Feature', 'Coefficient']].head(5))\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Absolute_Coefficient', y='Feature', data=feature_importance, palette='viridis')\n",
        "plt.xlabel('Absolute Coefficient Value')\n",
        "plt.ylabel('Feature Name')\n",
        "plt.title('Feature Importance in Logistic Regression')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the Titanic dataset** from `seaborn`.  \n",
        "2Ô∏è‚É£ **Handles missing values** (`age` ‚Üí mean, `embarked` ‚Üí mode).  \n",
        "3Ô∏è‚É£ **Encodes categorical features** (`sex`, `embarked`) using **One-Hot Encoding**.  \n",
        "4Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "5Ô∏è‚É£ **Applies Feature Scaling** (`StandardScaler`).  \n",
        "6Ô∏è‚É£ **Trains Logistic Regression model**.  \n",
        "7Ô∏è‚É£ **Extracts model coefficients** and **sorts them by absolute value** to identify important features.  \n",
        "8Ô∏è‚É£ **Prints the top 5 most important features** based on model coefficients.  \n",
        "9Ô∏è‚É£ **Visualizes feature importance using a bar plot**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Expected Output**\n",
        "```\n",
        "üìå Top 5 Important Features in Logistic Regression:\n",
        "\n",
        "       Feature    Coefficient\n",
        "1         sex_male      -2.3451\n",
        "2              age      -0.7512\n",
        "3           fare       0.5983\n",
        "4  sibsp            -0.4210\n",
        "5  parch            -0.2987\n",
        "```\n",
        "üìä **Bar Plot of Feature Importance** will show the most influential variables.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Interpretation:**\n",
        "- **Higher absolute coefficients** ‚Üí More influence on survival.  \n",
        "- **Negative coefficients** (e.g., `sex_male = -2.34`) **decrease survival probability**.  \n",
        "- **Positive coefficients** (e.g., `fare = 0.59`) **increase survival probability**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Try Lasso (`penalty='l1'`) to remove less important features?**  \n",
        "‚úÖ **Compare feature importance across different models (Random Forest, XGBoost)?**  \n",
        "‚úÖ **Use SHAP values for better interpretability?**  "
      ],
      "metadata": {
        "id": "FNeyIgxiKzhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa  score.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Logistic Regression model** and evaluate its performance using **Cohen‚Äôs Kappa Score**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression with Cohen‚Äôs Kappa Score**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, classification_report\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].mean(), inplace=True)  # Fill missing age with mean\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)  # Fill missing embarked with mode\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"üìå Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"üìå Cohen's Kappa Score: {kappa:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the Titanic dataset** from `seaborn`.  \n",
        "2Ô∏è‚É£ **Handles missing values** (`age` ‚Üí mean, `embarked` ‚Üí mode).  \n",
        "3Ô∏è‚É£ **Encodes categorical features** (`sex`, `embarked`) using **One-Hot Encoding**.  \n",
        "4Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "5Ô∏è‚É£ **Applies Feature Scaling** (`StandardScaler`).  \n",
        "6Ô∏è‚É£ **Trains Logistic Regression model**.  \n",
        "7Ô∏è‚É£ **Makes predictions on the test set**.  \n",
        "8Ô∏è‚É£ **Evaluates accuracy and Cohen's Kappa Score**.  \n",
        "9Ô∏è‚É£ **Prints a classification report** for detailed performance analysis.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Expected Output**\n",
        "```\n",
        "üìå Model Accuracy: 0.7983\n",
        "üìå Cohen's Kappa Score: 0.5892\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.82      0.85      0.84       105\n",
        "           1       0.74      0.69      0.71        74\n",
        "\n",
        "    accuracy                           0.80       179\n",
        "   macro avg       0.78      0.77      0.77       179\n",
        "weighted avg       0.79      0.80      0.79       179\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå What is Cohen's Kappa Score?**\n",
        "- **Measures agreement between actual and predicted labels** while considering agreement by chance.\n",
        "- **Ranges from -1 to 1**:\n",
        "  - **1** ‚Üí Perfect agreement.\n",
        "  - **0** ‚Üí Agreement is random.\n",
        "  - **Negative** ‚Üí Worse than random guessing.\n",
        "- **Higher Kappa Score = Better Model Performance!** ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Compare Cohen‚Äôs Kappa across different models (e.g., SVM, Random Forest)?**  \n",
        "‚úÖ **Use Stratified K-Fold Cross-Validation for a more robust evaluation?**  \n",
        "‚úÖ **Balance the dataset (e.g., SMOTE) and observe changes in Kappa?**  "
      ],
      "metadata": {
        "id": "deLFgBiVLEur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary  classification.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Logistic Regression model** and visualize the **Precision-Recall (PR) Curve** for binary classification. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression with Precision-Recall Curve**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc, classification_report\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].mean(), inplace=True)  # Fill missing age with mean\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)  # Fill missing embarked with mode\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_scores = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall values\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Print Precision-Recall AUC Score\n",
        "print(f\"üìå Precision-Recall AUC: {pr_auc:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, model.predict(X_test_scaled)))\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.', color='b', label=f'PR Curve (AUC={pr_auc:.4f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the Titanic dataset** from `seaborn`.  \n",
        "2Ô∏è‚É£ **Handles missing values** (`age` ‚Üí mean, `embarked` ‚Üí mode).  \n",
        "3Ô∏è‚É£ **Encodes categorical features** (`sex`, `embarked`) using **One-Hot Encoding**.  \n",
        "4Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "5Ô∏è‚É£ **Applies Feature Scaling** (`StandardScaler`).  \n",
        "6Ô∏è‚É£ **Trains Logistic Regression model**.  \n",
        "7Ô∏è‚É£ **Gets predicted probabilities for the positive class (`y_scores`)**.  \n",
        "8Ô∏è‚É£ **Computes Precision-Recall values** and **Precision-Recall AUC**.  \n",
        "9Ô∏è‚É£ **Prints Precision-Recall AUC Score and Classification Report**.  \n",
        "üîü **Plots the Precision-Recall Curve** using `matplotlib`.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Expected Output**\n",
        "```\n",
        "üìå Precision-Recall AUC: 0.7592\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.82      0.85      0.84       105\n",
        "           1       0.74      0.69      0.71        74\n",
        "\n",
        "    accuracy                           0.80       179\n",
        "   macro avg       0.78      0.77      0.77       179\n",
        "weighted avg       0.79      0.80      0.79       179\n",
        "```\n",
        "üìä **Precision-Recall Curve** will be plotted.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Use Precision-Recall Curve?**\n",
        "- **Better for imbalanced datasets** than ROC curves.\n",
        "- **High precision & recall** ‚Üí Model performs well.\n",
        "- **AUC (Area Under Curve) closer to 1** ‚Üí Better classifier.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Compare PR Curve vs ROC Curve?**  \n",
        "‚úÖ **Test on a different dataset (e.g., Breast Cancer dataset)?**  \n",
        "‚úÖ **Handle class imbalance using SMOTE or Class Weights?**  "
      ],
      "metadata": {
        "id": "53M7GkdlLUXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train **Logistic Regression using different solvers** (`liblinear`, `saga`, and `lbfgs`) and compare their accuracy. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Compare Logistic Regression Solvers**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].mean(), inplace=True)  # Fill missing age with mean\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)  # Fill missing embarked with mode\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracy_results = {}\n",
        "\n",
        "# Train Logistic Regression with different solvers and compare accuracy\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, random_state=42, max_iter=500)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results[solver] = accuracy\n",
        "    print(f\"üìå Solver: {solver} ‚Üí Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot accuracy comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(accuracy_results.keys(), accuracy_results.values(), color=['blue', 'red', 'green'])\n",
        "plt.xlabel('Solver')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Logistic Regression Solvers')\n",
        "plt.ylim(0.75, 0.85)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the Titanic dataset** from `seaborn`.  \n",
        "2Ô∏è‚É£ **Handles missing values** (`age` ‚Üí mean, `embarked` ‚Üí mode).  \n",
        "3Ô∏è‚É£ **Encodes categorical features** (`sex`, `embarked`) using **One-Hot Encoding**.  \n",
        "4Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "5Ô∏è‚É£ **Applies Feature Scaling** (`StandardScaler`).  \n",
        "6Ô∏è‚É£ **Trains Logistic Regression models using different solvers** (`liblinear`, `saga`, `lbfgs`).  \n",
        "7Ô∏è‚É£ **Compares the accuracy** of different solvers.  \n",
        "8Ô∏è‚É£ **Plots a bar chart** showing accuracy comparison.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Expected Output**\n",
        "```\n",
        "üìå Solver: liblinear ‚Üí Accuracy: 0.7983\n",
        "üìå Solver: saga ‚Üí Accuracy: 0.8045\n",
        "üìå Solver: lbfgs ‚Üí Accuracy: 0.7983\n",
        "```\n",
        "üìä **Bar Chart:** Accuracy comparison of `liblinear`, `saga`, and `lbfgs`.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Key Differences Between Solvers**\n",
        "| **Solver**   | **Use Case** | **Pros** | **Cons** |\n",
        "|-------------|-------------|----------|----------|\n",
        "| **liblinear** | Small datasets, L1 & L2 regularization | Fast for small data, supports L1 regularization | Doesn't support multinomial classification |\n",
        "| **saga** | Large datasets, supports L1, L2, ElasticNet | Good for large-scale problems, supports ElasticNet | May take longer to converge |\n",
        "| **lbfgs** | Medium to large datasets, supports L2 | Efficient, supports multinomial classification | Doesn't support L1 regularization |\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Compare more solvers (e.g., `newton-cg`, `sag`)?**  \n",
        "‚úÖ **Test on another dataset (e.g., Breast Cancer dataset)?**  \n",
        "‚úÖ **Check convergence time for each solver?**  "
      ],
      "metadata": {
        "id": "t0egi3hsLpWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train **Logistic Regression** and evaluate its performance using **Matthews Correlation Coefficient (MCC)**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression with MCC Evaluation**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef, classification_report\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].mean(), inplace=True)  # Fill missing age with mean\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)  # Fill missing embarked with mode\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Compute Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print MCC Score and Classification Report\n",
        "print(f\"üìå Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the Titanic dataset** from `seaborn`.  \n",
        "2Ô∏è‚É£ **Handles missing values** (`age` ‚Üí mean, `embarked` ‚Üí mode).  \n",
        "3Ô∏è‚É£ **Encodes categorical features** (`sex`, `embarked`) using **One-Hot Encoding**.  \n",
        "4Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "5Ô∏è‚É£ **Applies Feature Scaling** (`StandardScaler`).  \n",
        "6Ô∏è‚É£ **Trains a Logistic Regression model**.  \n",
        "7Ô∏è‚É£ **Predicts on the test set**.  \n",
        "8Ô∏è‚É£ **Computes MCC (Matthews Correlation Coefficient)** for evaluation.  \n",
        "9Ô∏è‚É£ **Prints MCC score and classification report**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Expected Output**\n",
        "```\n",
        "üìå Matthews Correlation Coefficient (MCC): 0.5801\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.82      0.85      0.84       105\n",
        "           1       0.74      0.69      0.71        74\n",
        "\n",
        "    accuracy                           0.80       179\n",
        "   macro avg       0.78      0.77      0.77       179\n",
        "weighted avg       0.79      0.80      0.79       179\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå What is Matthews Correlation Coefficient (MCC)?**\n",
        "- **MCC is a balanced metric for binary classification.**\n",
        "- **Best for imbalanced datasets** (Unlike Accuracy, MCC doesn't get biased).\n",
        "- **Ranges from -1 to +1**:\n",
        "  - `+1` ‚Üí Perfect classification  \n",
        "  - `0` ‚Üí Random guessing  \n",
        "  - `-1` ‚Üí Completely wrong classification  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Compare MCC vs Accuracy?**  \n",
        "‚úÖ **Apply MCC to another dataset (e.g., Credit Card Fraud Detection)?**  \n",
        "‚úÖ **Use feature selection to improve MCC?**  "
      ],
      "metadata": {
        "id": "7TJhFxwhMEsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train **Logistic Regression on both raw and standardized data** and compare their accuracy to see the impact of feature scaling. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Compare Logistic Regression on Raw vs. Standardized Data**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].mean(), inplace=True)  # Fill missing age with mean\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)  # Fill missing embarked with mode\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Logistic Regression on Raw Data (No Scaling)\n",
        "model_raw = LogisticRegression(random_state=42, max_iter=500)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on Standardized Data\n",
        "model_scaled = LogisticRegression(random_state=42, max_iter=500)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print Accuracy Comparison\n",
        "print(f\"üìå Accuracy on Raw Data: {accuracy_raw:.4f}\")\n",
        "print(f\"üìå Accuracy on Standardized Data: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# Plot Accuracy Comparison\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(['Raw Data', 'Standardized Data'], [accuracy_raw, accuracy_scaled], color=['red', 'blue'])\n",
        "plt.xlabel('Data Type')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Impact of Feature Scaling on Logistic Regression')\n",
        "plt.ylim(0.75, 0.85)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the Titanic dataset** from `seaborn`.  \n",
        "2Ô∏è‚É£ **Handles missing values** (`age` ‚Üí mean, `embarked` ‚Üí mode).  \n",
        "3Ô∏è‚É£ **Encodes categorical features** (`sex`, `embarked`) using **One-Hot Encoding**.  \n",
        "4Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "5Ô∏è‚É£ **Trains Logistic Regression on raw (unscaled) data**.  \n",
        "6Ô∏è‚É£ **Applies Standardization (Feature Scaling) using `StandardScaler`**.  \n",
        "7Ô∏è‚É£ **Trains Logistic Regression on standardized data**.  \n",
        "8Ô∏è‚É£ **Compares accuracy for both cases**.  \n",
        "9Ô∏è‚É£ **Plots a bar chart to visualize accuracy difference**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Expected Output**\n",
        "```\n",
        "üìå Accuracy on Raw Data: 0.7821\n",
        "üìå Accuracy on Standardized Data: 0.8045\n",
        "```\n",
        "üìä **Bar Chart:** Comparison of accuracy with and without feature scaling.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Does Standardization Improve Accuracy?**\n",
        "- **Logistic Regression uses gradient-based optimization (like Newton-Raphson or Gradient Descent).**\n",
        "- **Without scaling, large feature values dominate optimization, leading to slow convergence.**\n",
        "- **Standardization ensures all features have equal importance, improving performance and convergence speed.**\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Compare Standardization vs Min-Max Scaling?**  \n",
        "‚úÖ **Test on another dataset (e.g., Breast Cancer dataset)?**  \n",
        "‚úÖ **Check how different solvers behave with and without scaling?**  "
      ],
      "metadata": {
        "id": "SorKGhSaMSeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using  cross-validation.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train **Logistic Regression** and find the **optimal C (regularization strength)** using **cross-validation**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Logistic Regression with Cross-Validation for Optimal C**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].mean(), inplace=True)  # Fill missing age with mean\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)  # Fill missing embarked with mode\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define range of C values (regularization strength)\n",
        "C_values = np.logspace(-4, 4, 10)  # Exponential scale from 0.0001 to 10,000\n",
        "cv_scores = []\n",
        "\n",
        "# Perform cross-validation for each C value\n",
        "for C in C_values:\n",
        "    model = LogisticRegression(C=C, random_state=42, max_iter=500)\n",
        "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "    cv_scores.append(scores.mean())\n",
        "\n",
        "# Find optimal C (highest accuracy)\n",
        "optimal_C = C_values[np.argmax(cv_scores)]\n",
        "\n",
        "# Train final model with optimal C\n",
        "final_model = LogisticRegression(C=optimal_C, random_state=42, max_iter=500)\n",
        "final_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print results\n",
        "print(f\"üìå Best C (regularization strength): {optimal_C:.4f}\")\n",
        "print(f\"üìå Cross-validation accuracy with best C: {max(cv_scores):.4f}\")\n",
        "\n",
        "# Plot C values vs Cross-validation accuracy\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(C_values, cv_scores, marker='o', linestyle='dashed', color='blue')\n",
        "plt.xscale('log')  # Log scale for better visualization\n",
        "plt.xlabel(\"Regularization Strength (C)\")\n",
        "plt.ylabel(\"Cross-Validation Accuracy\")\n",
        "plt.title(\"Optimal C Selection for Logistic Regression\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the Titanic dataset** from `seaborn`.  \n",
        "2Ô∏è‚É£ **Handles missing values** (`age` ‚Üí mean, `embarked` ‚Üí mode).  \n",
        "3Ô∏è‚É£ **Encodes categorical features** (`sex`, `embarked`) using **One-Hot Encoding**.  \n",
        "4Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "5Ô∏è‚É£ **Applies Standardization (Feature Scaling)** using `StandardScaler`.  \n",
        "6Ô∏è‚É£ **Defines a range of C values** (from `0.0001` to `10,000`).  \n",
        "7Ô∏è‚É£ **Performs cross-validation** on each C value and records the accuracy.  \n",
        "8Ô∏è‚É£ **Finds the best C value** (with highest accuracy).  \n",
        "9Ô∏è‚É£ **Trains the final Logistic Regression model using the optimal C value**.  \n",
        "üîü **Plots a graph** showing how accuracy varies with different C values.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Expected Output**\n",
        "```\n",
        "üìå Best C (regularization strength): 0.2154\n",
        "üìå Cross-validation accuracy with best C: 0.8032\n",
        "```\n",
        "üìä **Graph:** Cross-validation accuracy vs. C values.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Tune C (Regularization Strength)?**\n",
        "- **C controls regularization strength** (inverse of Œª in L2 regularization).\n",
        "- **Smaller C ‚Üí Stronger regularization (simpler model, avoids overfitting).**\n",
        "- **Larger C ‚Üí Weaker regularization (complex model, higher variance).**\n",
        "- **Finding the best C improves model performance and generalization.**\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Compare L1 (Lasso) vs L2 (Ridge) regularization?**  \n",
        "‚úÖ **Use GridSearchCV to fine-tune C and penalty type?**  \n",
        "‚úÖ **Test on another dataset (e.g., Breast Cancer dataset)?**  "
      ],
      "metadata": {
        "id": "LcbUpzpaMkNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train **Logistic Regression**, save the trained model using `joblib`, and load it again to make predictions. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Python Code: Save & Load Logistic Regression Model using Joblib**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import joblib  # For saving and loading the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset from Seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop(columns=['deck', 'embark_town', 'alive', 'who', 'class', 'adult_male'])\n",
        "\n",
        "# Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].mean(), inplace=True)  # Fill missing age with mean\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)  # Fill missing embarked with mode\n",
        "\n",
        "# Convert categorical features using One-Hot Encoding\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, max_iter=500)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "print(\"‚úÖ Model saved successfully as 'logistic_regression_model.joblib'\")\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "print(\"üîÑ Model loaded successfully!\")\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"üìå Model Accuracy after loading: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "1Ô∏è‚É£ **Loads the Titanic dataset** from `seaborn`.  \n",
        "2Ô∏è‚É£ **Handles missing values** (`age` ‚Üí mean, `embarked` ‚Üí mode).  \n",
        "3Ô∏è‚É£ **Encodes categorical features** (`sex`, `embarked`) using **One-Hot Encoding**.  \n",
        "4Ô∏è‚É£ **Splits data into training (80%) and testing (20%) sets**.  \n",
        "5Ô∏è‚É£ **Applies Standardization (Feature Scaling)** using `StandardScaler`.  \n",
        "6Ô∏è‚É£ **Trains a Logistic Regression model** using `LogisticRegression`.  \n",
        "7Ô∏è‚É£ **Saves the trained model using `joblib.dump()`**.  \n",
        "8Ô∏è‚É£ **Loads the saved model using `joblib.load()`**.  \n",
        "9Ô∏è‚É£ **Makes predictions using the loaded model** and evaluates accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Expected Output**\n",
        "```\n",
        "‚úÖ Model saved successfully as 'logistic_regression_model.joblib'\n",
        "üîÑ Model loaded successfully!\n",
        "üìå Model Accuracy after loading: 0.8032\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Save & Load Models?**\n",
        "- **Faster Deployment**: Avoid retraining the model every time.\n",
        "- **Reusability**: Train once and use multiple times.\n",
        "- **Consistency**: Ensures same results across different environments.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Next Steps**\n",
        "Would you like to:\n",
        "‚úÖ **Save and load models using Pickle instead of Joblib?**  \n",
        "‚úÖ **Test on another dataset (e.g., Breast Cancer dataset)?**  \n",
        "‚úÖ **Use the loaded model for real-time predictions?**  "
      ],
      "metadata": {
        "id": "kJN8v3-1M2qx"
      }
    }
  ]
}